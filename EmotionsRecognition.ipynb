{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionsRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "CjWvnaQUrZmD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Emotion classification using the RAVDESS dataset"
      ]
    },
    {
      "metadata": {
        "id": "ldtHMhuLrewK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is licensed under CC BY-NA-SC 4.0. and can be downloaded free of charge at https://zenodo.org/record/1188976.\n",
        "\n",
        "***Construction and Validation***\n",
        "\n",
        "Construction and validation of the RAVDESS is described in our paper: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
        "\n",
        "The RAVDESS contains 7356 files. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLOS ONE.\n",
        "\n",
        "***Description***\n",
        "\n",
        "The dataset contains the complete set of 7356 RAVDESS files (total size: 24.8 GB). Each of the 24 actors consists of three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound).  Note, there are no song files for Actor_18.\n",
        "\n",
        "***Data***\n",
        "\n",
        "For this task, I have used 4948 samples from the RAVDESS dataset.\n",
        "\n",
        "The samples comes from:\n",
        "\n",
        "- Audio-only files;\n",
        "- Video + audio files: I have extracted the audio from each file using the script Mp4ToWav.py that you can find in the main directory of the project.\n",
        "\n",
        "***License information***\n",
        "\n",
        "The RAVDESS is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License, CC BY-NA-SC 4.0\n",
        "\n",
        "***File naming convention***\n",
        "\n",
        "Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:\n",
        "\n",
        "***Filename identifiers***\n",
        "\n",
        "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "- Vocal channel (01 = speech, 02 = song).\n",
        "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
        "- Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
        "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "Filename example: 02-01-06-01-02-01-12.mp4 \n",
        "\n",
        "- Video-only (02)\n",
        "- Speech (01)\n",
        "- Fearful (06)\n",
        "- Normal intensity (01)\n",
        "- Statement “dogs” (02)\n",
        "- 1st Repetition (01)\n",
        "- 12th Actor (12)\n",
        "- Female, as the actor ID number is even."
      ]
    },
    {
      "metadata": {
        "id": "JDNbxj45rkvB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "We are using Colab, a Google Cloud environment for jupyter, so we need to import our files from Google Drive and then install LibROSA, a python package for music and audio analysis.\n",
        "\n",
        "After the import, we will plot the signal of the first file."
      ]
    },
    {
      "metadata": {
        "id": "N-o2JI49WBAe",
        "colab_type": "code",
        "outputId": "2acfd105-912b-4bfb-c957-14c8da3a78c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EgFwaDhMbJVm",
        "colab_type": "code",
        "outputId": "d0815d16-1327-439b-bcc8-fc127b8c7a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.6)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.20.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.3.2)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.11.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.40.1)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.27.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rxI4xzngdS-e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "\n",
        "data, sampling_rate = librosa.load('/content/drive/My Drive/Ravdess/03-01-01-01-01-01-01.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgaSHtCIdtX2",
        "colab_type": "code",
        "outputId": "31e86ba7-200c-4df4-ba93-8d1b99d22531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "% pylab inline\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob \n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.waveplot(data, sr=sampling_rate)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['display']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PolyCollection at 0x7f755c6d6a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAEGCAYAAABxSsNVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vCtNuVWlr5jL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load all files\n",
        "\n",
        "We will create our numpy array extracting Mel-frequency cepstral coefficients (MFCCs), while the classes to predict will be extracted from the name of the file (see the introductory section of this notebook to see the naming convention of the files of this dataset)."
      ]
    },
    {
      "metadata": {
        "id": "AKvuF--gd6F-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = '/content/drive/My Drive/Ravdess'\n",
        "lst = []\n",
        "\n",
        "for subdir, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "      try:\n",
        "        #Load librosa array, obtain mfcss, store the file and the mcss information in a new array\n",
        "        X, sample_rate = librosa.load(os.path.join(subdir,file), res_type='kaiser_fast')\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "        file = file[6:8]\n",
        "        arr = mfccs, file\n",
        "        lst.append(arr)\n",
        "      # If the file is not valid, skip it\n",
        "      except ValueError:\n",
        "        continue       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kLSggnF7kKY1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating X and y: zip makes a list of all the first elements, and a list of all the second elements.\n",
        "X, y = zip(*lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VzvBRTJIlIE9",
        "colab_type": "code",
        "outputId": "c417e406-d27f-43b1-a2ed-985ed8132b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y)\n",
        "\n",
        "\n",
        "X.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4948, 40), (4948,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "Agw-3KN1sDhh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier\n",
        "\n",
        "To make a first attempt in accomplishing this classification task I chose a decision tree:"
      ]
    },
    {
      "metadata": {
        "id": "Q-Xgb5NslTBO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UshLOC1ClWL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_BnCR52nlXw0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dtree = DecisionTreeClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qWyTownblZM0",
        "colab_type": "code",
        "outputId": "e4d44d24-cd53-4b3a-d569-d34fc70e866e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "dtree.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "            max_features=None, max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
              "            splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "HEuw6TUQlr7C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = dtree.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_1v0i0V7sMw7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's go with our classification report.\n",
        "\n",
        "Before we start, a quick reminder of the classes we are trying to predict:\n",
        "\n",
        "emotions = {\n",
        "    \"neutral\": \"01\",\n",
        "    \"calm\": \"02\",\n",
        "    \"happy\": \"03\",\n",
        "    \"sad\": \"04\",\n",
        "    \"angry\": \"05\", \n",
        "    \"fearful\": \"06\", \n",
        "    \"disgust\": \"07\", \n",
        "    \"surprised\": \"08\"\n",
        "}"
      ]
    },
    {
      "metadata": {
        "id": "c4kNSYkAleIv",
        "colab_type": "code",
        "outputId": "6c5b61bf-3add-42e1-af65-9484c1bf7b36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          01       0.78      0.80      0.79       134\n",
            "          02       0.89      0.86      0.87       251\n",
            "          03       0.80      0.70      0.75       242\n",
            "          04       0.77      0.75      0.76       271\n",
            "          05       0.83      0.86      0.84       253\n",
            "          06       0.75      0.82      0.78       239\n",
            "          07       0.69      0.70      0.70       127\n",
            "          08       0.71      0.78      0.74       116\n",
            "\n",
            "   micro avg       0.79      0.79      0.79      1633\n",
            "   macro avg       0.78      0.78      0.78      1633\n",
            "weighted avg       0.79      0.79      0.79      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lCVgjLj-gwE2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ]
    },
    {
      "metadata": {
        "id": "jfaTxzZ1w__y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this second approach, I switched to a random forest classifier and I made a gridsearch to make some hyperparameters tuning.\n",
        "\n",
        "The gridsearch is not shown in the code below otherwise the notebook will require too much time to run."
      ]
    },
    {
      "metadata": {
        "id": "wcov_DCXgs7v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3eo0ljqzg-KM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rforest = RandomForestClassifier(criterion=\"gini\", max_depth=10, max_features=\"log2\", \n",
        "                                 max_leaf_nodes = 100, min_samples_leaf = 3, min_samples_split = 20, \n",
        "                                 n_estimators= 22000, random_state= 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tg45qSOfg-26",
        "colab_type": "code",
        "outputId": "89577ee4-698f-4bff-aaf7-f5ddcb4b3633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "rforest.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "            max_depth=10, max_features='log2', max_leaf_nodes=100,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=3, min_samples_split=20,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=22000, n_jobs=None,\n",
              "            oob_score=False, random_state=5, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "aM8KU3qxhGBM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = rforest.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "296FW5sBdanI",
        "colab_type": "code",
        "outputId": "c6e5c22a-1088-4658-cde7-a07326fcd2cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          01       1.00      0.54      0.70       134\n",
            "          02       0.66      0.96      0.78       251\n",
            "          03       0.86      0.71      0.78       242\n",
            "          04       0.81      0.64      0.71       271\n",
            "          05       0.89      0.88      0.88       253\n",
            "          06       0.70      0.80      0.75       239\n",
            "          07       0.73      0.61      0.66       127\n",
            "          08       0.60      0.78      0.68       116\n",
            "\n",
            "   micro avg       0.76      0.76      0.76      1633\n",
            "   macro avg       0.78      0.74      0.74      1633\n",
            "weighted avg       0.79      0.76      0.76      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t9eqMHV3S8i6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural network"
      ]
    },
    {
      "metadata": {
        "id": "G-QscoyMxQtn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's build our neural network!\n",
        "\n",
        "To do so, we need to expand the dimensions of our array, adding a third one using the numpy \"expand_dims\" feature."
      ]
    },
    {
      "metadata": {
        "id": "W4i187-Pe-w5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_traincnn = np.expand_dims(X_train, axis=2)\n",
        "x_testcnn = np.expand_dims(X_test, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnvoCRX1gQCh",
        "colab_type": "code",
        "outputId": "be26953c-b43b-4d0b-c78f-d5a10574bee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_traincnn.shape, x_testcnn.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3315, 40, 1), (1633, 40, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "HZOGIpuefCd3",
        "colab_type": "code",
        "outputId": "1a3a449c-9719-4379-98e5-e4aa422f41fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "import keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(128, 5,padding='same',\n",
        "                 input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 5,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(9))\n",
        "model.add(Activation('softmax'))\n",
        "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LphftMIZzUvz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With *model.summary* we can see a recap of what we have build:"
      ]
    },
    {
      "metadata": {
        "id": "pIWPB4Zgfic7",
        "colab_type": "code",
        "outputId": "64039473-24eb-4cc2-8492-48c15383ffb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 9)                 5769      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 9)                 0         \n",
            "=================================================================\n",
            "Total params: 88,585\n",
            "Trainable params: 88,585\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5qQSBeBhzcLu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can compile and fit our model:"
      ]
    },
    {
      "metadata": {
        "id": "iNI1znbsfpTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktdF-nJKfq6F",
        "colab_type": "code",
        "outputId": "2153fb8c-3984-460a-89e9-f646580f5239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34034
        }
      },
      "cell_type": "code",
      "source": [
        "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3315 samples, validate on 1633 samples\n",
            "Epoch 1/1000\n",
            "3315/3315 [==============================] - 2s 550us/step - loss: 9.9233 - acc: 0.1379 - val_loss: 7.1518 - val_acc: 0.1715\n",
            "Epoch 2/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 8.0147 - acc: 0.1478 - val_loss: 4.6204 - val_acc: 0.1647\n",
            "Epoch 3/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 6.5113 - acc: 0.1581 - val_loss: 2.5976 - val_acc: 0.1886\n",
            "Epoch 4/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 4.1797 - acc: 0.1698 - val_loss: 2.3895 - val_acc: 0.1672\n",
            "Epoch 5/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 3.2711 - acc: 0.1858 - val_loss: 2.1951 - val_acc: 0.2217\n",
            "Epoch 6/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 2.6262 - acc: 0.2063 - val_loss: 2.0669 - val_acc: 0.2070\n",
            "Epoch 7/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 2.2696 - acc: 0.2223 - val_loss: 1.8963 - val_acc: 0.2407\n",
            "Epoch 8/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 2.0662 - acc: 0.2293 - val_loss: 1.7854 - val_acc: 0.3154\n",
            "Epoch 9/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 1.9764 - acc: 0.2425 - val_loss: 1.8363 - val_acc: 0.2774\n",
            "Epoch 10/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 1.9107 - acc: 0.2682 - val_loss: 1.7502 - val_acc: 0.3778\n",
            "Epoch 11/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 1.8579 - acc: 0.2664 - val_loss: 1.7689 - val_acc: 0.3111\n",
            "Epoch 12/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 1.8266 - acc: 0.2784 - val_loss: 1.7491 - val_acc: 0.2933\n",
            "Epoch 13/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 1.7807 - acc: 0.3056 - val_loss: 1.6805 - val_acc: 0.3980\n",
            "Epoch 14/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 1.7336 - acc: 0.3327 - val_loss: 1.6404 - val_acc: 0.4133\n",
            "Epoch 15/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 1.7262 - acc: 0.3252 - val_loss: 1.6275 - val_acc: 0.4262\n",
            "Epoch 16/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 1.7035 - acc: 0.3439 - val_loss: 1.6277 - val_acc: 0.3686\n",
            "Epoch 17/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 1.6742 - acc: 0.3632 - val_loss: 1.6024 - val_acc: 0.4446\n",
            "Epoch 18/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 1.6438 - acc: 0.3786 - val_loss: 1.5660 - val_acc: 0.4060\n",
            "Epoch 19/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 1.6220 - acc: 0.3819 - val_loss: 1.5446 - val_acc: 0.4513\n",
            "Epoch 20/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 1.6161 - acc: 0.3879 - val_loss: 1.5279 - val_acc: 0.4342\n",
            "Epoch 21/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 1.5927 - acc: 0.3940 - val_loss: 1.5257 - val_acc: 0.4562\n",
            "Epoch 22/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 1.5873 - acc: 0.4012 - val_loss: 1.5069 - val_acc: 0.4483\n",
            "Epoch 23/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 1.5540 - acc: 0.4172 - val_loss: 1.4921 - val_acc: 0.4746\n",
            "Epoch 24/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 1.5446 - acc: 0.4066 - val_loss: 1.4668 - val_acc: 0.4874\n",
            "Epoch 25/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 1.5164 - acc: 0.4308 - val_loss: 1.4598 - val_acc: 0.4691\n",
            "Epoch 26/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 1.4958 - acc: 0.4317 - val_loss: 1.4315 - val_acc: 0.5003\n",
            "Epoch 27/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 1.5026 - acc: 0.4314 - val_loss: 1.4330 - val_acc: 0.4966\n",
            "Epoch 28/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 1.4761 - acc: 0.4549 - val_loss: 1.4110 - val_acc: 0.4991\n",
            "Epoch 29/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 1.4543 - acc: 0.4558 - val_loss: 1.4237 - val_acc: 0.4832\n",
            "Epoch 30/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 1.4454 - acc: 0.4685 - val_loss: 1.3871 - val_acc: 0.4954\n",
            "Epoch 31/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 1.4272 - acc: 0.4724 - val_loss: 1.3631 - val_acc: 0.5242\n",
            "Epoch 32/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 1.4092 - acc: 0.4694 - val_loss: 1.3618 - val_acc: 0.5077\n",
            "Epoch 33/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 1.4042 - acc: 0.4784 - val_loss: 1.3620 - val_acc: 0.5089\n",
            "Epoch 34/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 1.3972 - acc: 0.4887 - val_loss: 1.3335 - val_acc: 0.5113\n",
            "Epoch 35/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 1.3871 - acc: 0.4839 - val_loss: 1.3443 - val_acc: 0.5077\n",
            "Epoch 36/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 1.3756 - acc: 0.4872 - val_loss: 1.3434 - val_acc: 0.5138\n",
            "Epoch 37/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 1.3608 - acc: 0.4986 - val_loss: 1.3167 - val_acc: 0.5401\n",
            "Epoch 38/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 1.3405 - acc: 0.5110 - val_loss: 1.2916 - val_acc: 0.5346\n",
            "Epoch 39/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 1.3374 - acc: 0.5074 - val_loss: 1.3200 - val_acc: 0.5162\n",
            "Epoch 40/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 1.3232 - acc: 0.5110 - val_loss: 1.2855 - val_acc: 0.5450\n",
            "Epoch 41/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 1.3148 - acc: 0.5119 - val_loss: 1.2963 - val_acc: 0.5266\n",
            "Epoch 42/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 1.3065 - acc: 0.5192 - val_loss: 1.2840 - val_acc: 0.5352\n",
            "Epoch 43/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 1.3054 - acc: 0.5119 - val_loss: 1.2712 - val_acc: 0.5475\n",
            "Epoch 44/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 1.2905 - acc: 0.5158 - val_loss: 1.2849 - val_acc: 0.5138\n",
            "Epoch 45/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 1.2907 - acc: 0.5249 - val_loss: 1.2707 - val_acc: 0.5346\n",
            "Epoch 46/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 1.2844 - acc: 0.5173 - val_loss: 1.2488 - val_acc: 0.5517\n",
            "Epoch 47/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 1.2845 - acc: 0.5204 - val_loss: 1.2415 - val_acc: 0.5548\n",
            "Epoch 48/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 1.2640 - acc: 0.5324 - val_loss: 1.2276 - val_acc: 0.5554\n",
            "Epoch 49/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 1.2621 - acc: 0.5363 - val_loss: 1.2169 - val_acc: 0.5628\n",
            "Epoch 50/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 1.2411 - acc: 0.5406 - val_loss: 1.2343 - val_acc: 0.5499\n",
            "Epoch 51/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 1.2430 - acc: 0.5288 - val_loss: 1.2307 - val_acc: 0.5597\n",
            "Epoch 52/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 1.2276 - acc: 0.5454 - val_loss: 1.2204 - val_acc: 0.5609\n",
            "Epoch 53/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 1.2277 - acc: 0.5478 - val_loss: 1.1822 - val_acc: 0.5750\n",
            "Epoch 54/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 1.2282 - acc: 0.5511 - val_loss: 1.1978 - val_acc: 0.5689\n",
            "Epoch 55/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 1.2176 - acc: 0.5548 - val_loss: 1.1838 - val_acc: 0.5750\n",
            "Epoch 56/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 1.2108 - acc: 0.5538 - val_loss: 1.2028 - val_acc: 0.5524\n",
            "Epoch 57/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 1.2026 - acc: 0.5475 - val_loss: 1.1788 - val_acc: 0.5762\n",
            "Epoch 58/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 1.2020 - acc: 0.5581 - val_loss: 1.1753 - val_acc: 0.5928\n",
            "Epoch 59/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 1.1838 - acc: 0.5689 - val_loss: 1.1584 - val_acc: 0.5756\n",
            "Epoch 60/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 1.1881 - acc: 0.5650 - val_loss: 1.1501 - val_acc: 0.5909\n",
            "Epoch 61/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 1.1823 - acc: 0.5638 - val_loss: 1.1549 - val_acc: 0.5762\n",
            "Epoch 62/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 1.1757 - acc: 0.5647 - val_loss: 1.1504 - val_acc: 0.5756\n",
            "Epoch 63/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 1.1548 - acc: 0.5837 - val_loss: 1.1568 - val_acc: 0.5885\n",
            "Epoch 64/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 1.1754 - acc: 0.5735 - val_loss: 1.1442 - val_acc: 0.5922\n",
            "Epoch 65/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 1.1480 - acc: 0.5759 - val_loss: 1.1332 - val_acc: 0.5879\n",
            "Epoch 66/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 1.1533 - acc: 0.5695 - val_loss: 1.1261 - val_acc: 0.5928\n",
            "Epoch 67/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 1.1445 - acc: 0.5744 - val_loss: 1.1304 - val_acc: 0.5915\n",
            "Epoch 68/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 1.1421 - acc: 0.5674 - val_loss: 1.1399 - val_acc: 0.5818\n",
            "Epoch 69/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 1.1416 - acc: 0.5756 - val_loss: 1.1179 - val_acc: 0.6038\n",
            "Epoch 70/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 1.1248 - acc: 0.5940 - val_loss: 1.1158 - val_acc: 0.5891\n",
            "Epoch 71/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 1.1231 - acc: 0.5867 - val_loss: 1.1028 - val_acc: 0.5934\n",
            "Epoch 72/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 1.1107 - acc: 0.5900 - val_loss: 1.1174 - val_acc: 0.5854\n",
            "Epoch 73/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 1.1094 - acc: 0.5903 - val_loss: 1.0862 - val_acc: 0.6111\n",
            "Epoch 74/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 1.0936 - acc: 0.5894 - val_loss: 1.0991 - val_acc: 0.6154\n",
            "Epoch 75/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 1.1068 - acc: 0.5816 - val_loss: 1.0919 - val_acc: 0.6050\n",
            "Epoch 76/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 1.0955 - acc: 0.5982 - val_loss: 1.0742 - val_acc: 0.6099\n",
            "Epoch 77/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 1.0879 - acc: 0.6054 - val_loss: 1.0658 - val_acc: 0.6173\n",
            "Epoch 78/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 1.0793 - acc: 0.6069 - val_loss: 1.1071 - val_acc: 0.6013\n",
            "Epoch 79/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 1.0803 - acc: 0.6000 - val_loss: 1.0714 - val_acc: 0.6289\n",
            "Epoch 80/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 1.0856 - acc: 0.6000 - val_loss: 1.0760 - val_acc: 0.6130\n",
            "Epoch 81/1000\n",
            "3315/3315 [==============================] - 2s 457us/step - loss: 1.0723 - acc: 0.6027 - val_loss: 1.0581 - val_acc: 0.6173\n",
            "Epoch 82/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 1.0726 - acc: 0.6121 - val_loss: 1.0706 - val_acc: 0.6118\n",
            "Epoch 83/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 1.0661 - acc: 0.6087 - val_loss: 1.0933 - val_acc: 0.6105\n",
            "Epoch 84/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 1.0615 - acc: 0.6051 - val_loss: 1.0560 - val_acc: 0.6160\n",
            "Epoch 85/1000\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 1.0483 - acc: 0.6211 - val_loss: 1.0626 - val_acc: 0.6191\n",
            "Epoch 86/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 1.0494 - acc: 0.6157 - val_loss: 1.0563 - val_acc: 0.6118\n",
            "Epoch 87/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 1.0513 - acc: 0.6084 - val_loss: 1.0496 - val_acc: 0.6185\n",
            "Epoch 88/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 1.0457 - acc: 0.6265 - val_loss: 1.0580 - val_acc: 0.6093\n",
            "Epoch 89/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 1.0309 - acc: 0.6172 - val_loss: 1.0472 - val_acc: 0.6111\n",
            "Epoch 90/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 1.0338 - acc: 0.6217 - val_loss: 1.0253 - val_acc: 0.6314\n",
            "Epoch 91/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 1.0182 - acc: 0.6238 - val_loss: 1.0141 - val_acc: 0.6289\n",
            "Epoch 92/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 1.0135 - acc: 0.6368 - val_loss: 1.0479 - val_acc: 0.6160\n",
            "Epoch 93/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 1.0218 - acc: 0.6281 - val_loss: 1.0190 - val_acc: 0.6479\n",
            "Epoch 94/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 1.0212 - acc: 0.6181 - val_loss: 1.0165 - val_acc: 0.6332\n",
            "Epoch 95/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 1.0096 - acc: 0.6341 - val_loss: 1.0146 - val_acc: 0.6271\n",
            "Epoch 96/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 1.0165 - acc: 0.6338 - val_loss: 1.0050 - val_acc: 0.6344\n",
            "Epoch 97/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 1.0127 - acc: 0.6268 - val_loss: 1.0102 - val_acc: 0.6381\n",
            "Epoch 98/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 1.0087 - acc: 0.6350 - val_loss: 1.0361 - val_acc: 0.6314\n",
            "Epoch 99/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.9915 - acc: 0.6359 - val_loss: 1.0130 - val_acc: 0.6265\n",
            "Epoch 100/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.9947 - acc: 0.6465 - val_loss: 0.9946 - val_acc: 0.6454\n",
            "Epoch 101/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.9872 - acc: 0.6404 - val_loss: 1.0074 - val_acc: 0.6412\n",
            "Epoch 102/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.9904 - acc: 0.6332 - val_loss: 0.9873 - val_acc: 0.6405\n",
            "Epoch 103/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.9701 - acc: 0.6446 - val_loss: 1.0058 - val_acc: 0.6307\n",
            "Epoch 104/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.9803 - acc: 0.6428 - val_loss: 0.9741 - val_acc: 0.6516\n",
            "Epoch 105/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.9752 - acc: 0.6392 - val_loss: 0.9725 - val_acc: 0.6418\n",
            "Epoch 106/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.9601 - acc: 0.6437 - val_loss: 0.9651 - val_acc: 0.6595\n",
            "Epoch 107/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.9555 - acc: 0.6456 - val_loss: 0.9722 - val_acc: 0.6638\n",
            "Epoch 108/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.9522 - acc: 0.6567 - val_loss: 0.9636 - val_acc: 0.6454\n",
            "Epoch 109/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.9627 - acc: 0.6459 - val_loss: 0.9497 - val_acc: 0.6632\n",
            "Epoch 110/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.9477 - acc: 0.6534 - val_loss: 0.9548 - val_acc: 0.6663\n",
            "Epoch 111/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.9582 - acc: 0.6504 - val_loss: 0.9610 - val_acc: 0.6503\n",
            "Epoch 112/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.9376 - acc: 0.6637 - val_loss: 0.9761 - val_acc: 0.6522\n",
            "Epoch 113/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.9362 - acc: 0.6652 - val_loss: 0.9483 - val_acc: 0.6699\n",
            "Epoch 114/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.9397 - acc: 0.6600 - val_loss: 0.9449 - val_acc: 0.6601\n",
            "Epoch 115/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.9415 - acc: 0.6567 - val_loss: 0.9626 - val_acc: 0.6522\n",
            "Epoch 116/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.9261 - acc: 0.6627 - val_loss: 0.9619 - val_acc: 0.6503\n",
            "Epoch 117/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.9326 - acc: 0.6564 - val_loss: 0.9617 - val_acc: 0.6644\n",
            "Epoch 118/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.9260 - acc: 0.6576 - val_loss: 0.9502 - val_acc: 0.6528\n",
            "Epoch 119/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 0.9245 - acc: 0.6643 - val_loss: 0.9645 - val_acc: 0.6405\n",
            "Epoch 120/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.9212 - acc: 0.6679 - val_loss: 0.9545 - val_acc: 0.6491\n",
            "Epoch 121/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.9095 - acc: 0.6709 - val_loss: 0.9407 - val_acc: 0.6632\n",
            "Epoch 122/1000\n",
            "3315/3315 [==============================] - 2s 518us/step - loss: 0.9130 - acc: 0.6718 - val_loss: 0.9178 - val_acc: 0.6828\n",
            "Epoch 123/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.9139 - acc: 0.6664 - val_loss: 0.9545 - val_acc: 0.6565\n",
            "Epoch 124/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.9083 - acc: 0.6688 - val_loss: 0.9354 - val_acc: 0.6712\n",
            "Epoch 125/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.9008 - acc: 0.6748 - val_loss: 0.9094 - val_acc: 0.6803\n",
            "Epoch 126/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.8934 - acc: 0.6733 - val_loss: 0.9357 - val_acc: 0.6699\n",
            "Epoch 127/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.8955 - acc: 0.6745 - val_loss: 0.9233 - val_acc: 0.6626\n",
            "Epoch 128/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.8750 - acc: 0.6772 - val_loss: 0.9146 - val_acc: 0.6810\n",
            "Epoch 129/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.8899 - acc: 0.6627 - val_loss: 0.9193 - val_acc: 0.6607\n",
            "Epoch 130/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.8837 - acc: 0.6745 - val_loss: 0.9150 - val_acc: 0.6724\n",
            "Epoch 131/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.8755 - acc: 0.6836 - val_loss: 0.9141 - val_acc: 0.6791\n",
            "Epoch 132/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.8776 - acc: 0.6745 - val_loss: 0.9303 - val_acc: 0.6638\n",
            "Epoch 133/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.8737 - acc: 0.6781 - val_loss: 0.9581 - val_acc: 0.6289\n",
            "Epoch 134/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.8790 - acc: 0.6857 - val_loss: 0.9167 - val_acc: 0.6797\n",
            "Epoch 135/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.8567 - acc: 0.6929 - val_loss: 0.9111 - val_acc: 0.6859\n",
            "Epoch 136/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.8622 - acc: 0.6866 - val_loss: 0.9077 - val_acc: 0.6828\n",
            "Epoch 137/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.8545 - acc: 0.6902 - val_loss: 0.9094 - val_acc: 0.6687\n",
            "Epoch 138/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.8501 - acc: 0.6923 - val_loss: 0.8871 - val_acc: 0.6767\n",
            "Epoch 139/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.8703 - acc: 0.6896 - val_loss: 0.8845 - val_acc: 0.6779\n",
            "Epoch 140/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.8558 - acc: 0.6896 - val_loss: 0.8803 - val_acc: 0.6871\n",
            "Epoch 141/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.8409 - acc: 0.6944 - val_loss: 0.9013 - val_acc: 0.6846\n",
            "Epoch 142/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.8590 - acc: 0.6824 - val_loss: 0.9076 - val_acc: 0.6509\n",
            "Epoch 143/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.8393 - acc: 0.6992 - val_loss: 0.9001 - val_acc: 0.6681\n",
            "Epoch 144/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.8453 - acc: 0.6953 - val_loss: 0.8891 - val_acc: 0.6810\n",
            "Epoch 145/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.8355 - acc: 0.6899 - val_loss: 0.8787 - val_acc: 0.6828\n",
            "Epoch 146/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.8390 - acc: 0.6974 - val_loss: 0.8989 - val_acc: 0.6675\n",
            "Epoch 147/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.8268 - acc: 0.6965 - val_loss: 0.9298 - val_acc: 0.6485\n",
            "Epoch 148/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.8331 - acc: 0.6965 - val_loss: 0.8637 - val_acc: 0.6926\n",
            "Epoch 149/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.8321 - acc: 0.6998 - val_loss: 0.8837 - val_acc: 0.6761\n",
            "Epoch 150/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.8248 - acc: 0.7023 - val_loss: 0.8680 - val_acc: 0.6901\n",
            "Epoch 151/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.8177 - acc: 0.7083 - val_loss: 0.8734 - val_acc: 0.6883\n",
            "Epoch 152/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.8159 - acc: 0.7068 - val_loss: 0.8628 - val_acc: 0.6816\n",
            "Epoch 153/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.8066 - acc: 0.7080 - val_loss: 0.8723 - val_acc: 0.6950\n",
            "Epoch 154/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.8163 - acc: 0.7068 - val_loss: 0.8675 - val_acc: 0.6871\n",
            "Epoch 155/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.8091 - acc: 0.7077 - val_loss: 0.8510 - val_acc: 0.6785\n",
            "Epoch 156/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.8088 - acc: 0.7062 - val_loss: 0.8482 - val_acc: 0.6950\n",
            "Epoch 157/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.8152 - acc: 0.7050 - val_loss: 0.8512 - val_acc: 0.6993\n",
            "Epoch 158/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.8064 - acc: 0.7128 - val_loss: 0.8383 - val_acc: 0.6932\n",
            "Epoch 159/1000\n",
            "3315/3315 [==============================] - 2s 517us/step - loss: 0.7959 - acc: 0.7086 - val_loss: 0.8856 - val_acc: 0.6822\n",
            "Epoch 160/1000\n",
            "3315/3315 [==============================] - 2s 502us/step - loss: 0.8029 - acc: 0.7107 - val_loss: 0.8524 - val_acc: 0.6877\n",
            "Epoch 161/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.7931 - acc: 0.7101 - val_loss: 0.8297 - val_acc: 0.6908\n",
            "Epoch 162/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.7835 - acc: 0.7225 - val_loss: 0.8830 - val_acc: 0.6773\n",
            "Epoch 163/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.7929 - acc: 0.7071 - val_loss: 0.8211 - val_acc: 0.7140\n",
            "Epoch 164/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.7983 - acc: 0.7134 - val_loss: 0.8357 - val_acc: 0.6963\n",
            "Epoch 165/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.7873 - acc: 0.7158 - val_loss: 0.8444 - val_acc: 0.6877\n",
            "Epoch 166/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.7833 - acc: 0.7107 - val_loss: 0.8169 - val_acc: 0.6957\n",
            "Epoch 167/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.7912 - acc: 0.7080 - val_loss: 0.8337 - val_acc: 0.7036\n",
            "Epoch 168/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.7857 - acc: 0.7149 - val_loss: 0.8192 - val_acc: 0.7073\n",
            "Epoch 169/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.7675 - acc: 0.7173 - val_loss: 0.8505 - val_acc: 0.6950\n",
            "Epoch 170/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.7804 - acc: 0.7213 - val_loss: 0.8382 - val_acc: 0.6883\n",
            "Epoch 171/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.7788 - acc: 0.7210 - val_loss: 0.8214 - val_acc: 0.7048\n",
            "Epoch 172/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.7691 - acc: 0.7222 - val_loss: 0.8825 - val_acc: 0.6852\n",
            "Epoch 173/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.7763 - acc: 0.7240 - val_loss: 0.8043 - val_acc: 0.7208\n",
            "Epoch 174/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.7597 - acc: 0.7237 - val_loss: 0.8179 - val_acc: 0.7085\n",
            "Epoch 175/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.7607 - acc: 0.7216 - val_loss: 0.8108 - val_acc: 0.7103\n",
            "Epoch 176/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.7549 - acc: 0.7330 - val_loss: 0.8122 - val_acc: 0.7097\n",
            "Epoch 177/1000\n",
            "3315/3315 [==============================] - 2s 485us/step - loss: 0.7534 - acc: 0.7297 - val_loss: 0.8222 - val_acc: 0.7067\n",
            "Epoch 178/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.7380 - acc: 0.7415 - val_loss: 0.8355 - val_acc: 0.6981\n",
            "Epoch 179/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.7566 - acc: 0.7246 - val_loss: 0.7938 - val_acc: 0.7097\n",
            "Epoch 180/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.7486 - acc: 0.7294 - val_loss: 0.8309 - val_acc: 0.7036\n",
            "Epoch 181/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.7471 - acc: 0.7376 - val_loss: 0.8127 - val_acc: 0.7042\n",
            "Epoch 182/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.7514 - acc: 0.7303 - val_loss: 0.8220 - val_acc: 0.6852\n",
            "Epoch 183/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.7453 - acc: 0.7231 - val_loss: 0.8306 - val_acc: 0.6975\n",
            "Epoch 184/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.7357 - acc: 0.7388 - val_loss: 0.7985 - val_acc: 0.7208\n",
            "Epoch 185/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.7274 - acc: 0.7403 - val_loss: 0.7970 - val_acc: 0.7103\n",
            "Epoch 186/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.7269 - acc: 0.7415 - val_loss: 0.7841 - val_acc: 0.7171\n",
            "Epoch 187/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.7309 - acc: 0.7318 - val_loss: 0.8037 - val_acc: 0.7036\n",
            "Epoch 188/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.7306 - acc: 0.7382 - val_loss: 0.7956 - val_acc: 0.7085\n",
            "Epoch 189/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.7369 - acc: 0.7430 - val_loss: 0.8020 - val_acc: 0.6987\n",
            "Epoch 190/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.7308 - acc: 0.7388 - val_loss: 0.8125 - val_acc: 0.7177\n",
            "Epoch 191/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.7256 - acc: 0.7345 - val_loss: 0.7879 - val_acc: 0.7299\n",
            "Epoch 192/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.7095 - acc: 0.7475 - val_loss: 0.7767 - val_acc: 0.7275\n",
            "Epoch 193/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.6984 - acc: 0.7457 - val_loss: 0.7717 - val_acc: 0.7189\n",
            "Epoch 194/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.7171 - acc: 0.7427 - val_loss: 0.7838 - val_acc: 0.7159\n",
            "Epoch 195/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.7220 - acc: 0.7318 - val_loss: 0.7700 - val_acc: 0.7287\n",
            "Epoch 196/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.7145 - acc: 0.7388 - val_loss: 0.7639 - val_acc: 0.7293\n",
            "Epoch 197/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.7246 - acc: 0.7363 - val_loss: 0.7865 - val_acc: 0.7146\n",
            "Epoch 198/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.7197 - acc: 0.7354 - val_loss: 0.7679 - val_acc: 0.7275\n",
            "Epoch 199/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.7018 - acc: 0.7460 - val_loss: 0.7698 - val_acc: 0.7281\n",
            "Epoch 200/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.7002 - acc: 0.7457 - val_loss: 0.7848 - val_acc: 0.7220\n",
            "Epoch 201/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.7055 - acc: 0.7412 - val_loss: 0.7738 - val_acc: 0.7171\n",
            "Epoch 202/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.7061 - acc: 0.7391 - val_loss: 0.7837 - val_acc: 0.7116\n",
            "Epoch 203/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.6935 - acc: 0.7535 - val_loss: 0.7998 - val_acc: 0.7122\n",
            "Epoch 204/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.7045 - acc: 0.7445 - val_loss: 0.7585 - val_acc: 0.7214\n",
            "Epoch 205/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.6865 - acc: 0.7551 - val_loss: 0.7594 - val_acc: 0.7385\n",
            "Epoch 206/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.6886 - acc: 0.7517 - val_loss: 0.7871 - val_acc: 0.7048\n",
            "Epoch 207/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.6913 - acc: 0.7532 - val_loss: 0.7600 - val_acc: 0.7355\n",
            "Epoch 208/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.6717 - acc: 0.7581 - val_loss: 0.7585 - val_acc: 0.7373\n",
            "Epoch 209/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.6817 - acc: 0.7551 - val_loss: 0.7841 - val_acc: 0.7128\n",
            "Epoch 210/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.6798 - acc: 0.7584 - val_loss: 0.7623 - val_acc: 0.7195\n",
            "Epoch 211/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.6791 - acc: 0.7578 - val_loss: 0.8069 - val_acc: 0.6999\n",
            "Epoch 212/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.6813 - acc: 0.7575 - val_loss: 0.7856 - val_acc: 0.7018\n",
            "Epoch 213/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.6751 - acc: 0.7578 - val_loss: 0.7443 - val_acc: 0.7361\n",
            "Epoch 214/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.6732 - acc: 0.7602 - val_loss: 0.7626 - val_acc: 0.7177\n",
            "Epoch 215/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.6785 - acc: 0.7551 - val_loss: 0.7432 - val_acc: 0.7397\n",
            "Epoch 216/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.6672 - acc: 0.7617 - val_loss: 0.7620 - val_acc: 0.7220\n",
            "Epoch 217/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.6585 - acc: 0.7611 - val_loss: 0.7274 - val_acc: 0.7367\n",
            "Epoch 218/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.6736 - acc: 0.7572 - val_loss: 0.7250 - val_acc: 0.7391\n",
            "Epoch 219/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.6606 - acc: 0.7590 - val_loss: 0.7561 - val_acc: 0.7226\n",
            "Epoch 220/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.6671 - acc: 0.7659 - val_loss: 0.7284 - val_acc: 0.7391\n",
            "Epoch 221/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.6483 - acc: 0.7692 - val_loss: 0.7312 - val_acc: 0.7532\n",
            "Epoch 222/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.6508 - acc: 0.7701 - val_loss: 0.7895 - val_acc: 0.7116\n",
            "Epoch 223/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.6664 - acc: 0.7653 - val_loss: 0.7444 - val_acc: 0.7404\n",
            "Epoch 224/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.6649 - acc: 0.7662 - val_loss: 0.7327 - val_acc: 0.7489\n",
            "Epoch 225/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.6447 - acc: 0.7677 - val_loss: 0.7370 - val_acc: 0.7416\n",
            "Epoch 226/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.6470 - acc: 0.7753 - val_loss: 0.7638 - val_acc: 0.7226\n",
            "Epoch 227/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.6369 - acc: 0.7722 - val_loss: 0.7500 - val_acc: 0.7281\n",
            "Epoch 228/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.6475 - acc: 0.7638 - val_loss: 0.7265 - val_acc: 0.7453\n",
            "Epoch 229/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.6606 - acc: 0.7650 - val_loss: 0.7154 - val_acc: 0.7532\n",
            "Epoch 230/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.6337 - acc: 0.7762 - val_loss: 0.7262 - val_acc: 0.7391\n",
            "Epoch 231/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.6436 - acc: 0.7656 - val_loss: 0.7391 - val_acc: 0.7361\n",
            "Epoch 232/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.6292 - acc: 0.7792 - val_loss: 0.7226 - val_acc: 0.7581\n",
            "Epoch 233/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.6228 - acc: 0.7852 - val_loss: 0.7173 - val_acc: 0.7495\n",
            "Epoch 234/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.6268 - acc: 0.7750 - val_loss: 0.7376 - val_acc: 0.7367\n",
            "Epoch 235/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.6196 - acc: 0.7792 - val_loss: 0.7225 - val_acc: 0.7446\n",
            "Epoch 236/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.6248 - acc: 0.7855 - val_loss: 0.7621 - val_acc: 0.7171\n",
            "Epoch 237/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.6149 - acc: 0.7732 - val_loss: 0.7156 - val_acc: 0.7502\n",
            "Epoch 238/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.6174 - acc: 0.7825 - val_loss: 0.7281 - val_acc: 0.7538\n",
            "Epoch 239/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.6256 - acc: 0.7774 - val_loss: 0.7078 - val_acc: 0.7508\n",
            "Epoch 240/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.6187 - acc: 0.7804 - val_loss: 0.7069 - val_acc: 0.7612\n",
            "Epoch 241/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.6150 - acc: 0.7795 - val_loss: 0.7039 - val_acc: 0.7477\n",
            "Epoch 242/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.6302 - acc: 0.7722 - val_loss: 0.6996 - val_acc: 0.7575\n",
            "Epoch 243/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.6136 - acc: 0.7885 - val_loss: 0.7100 - val_acc: 0.7557\n",
            "Epoch 244/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.6232 - acc: 0.7780 - val_loss: 0.6936 - val_acc: 0.7667\n",
            "Epoch 245/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.6082 - acc: 0.7852 - val_loss: 0.7121 - val_acc: 0.7410\n",
            "Epoch 246/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.6048 - acc: 0.7801 - val_loss: 0.7076 - val_acc: 0.7587\n",
            "Epoch 247/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.6165 - acc: 0.7798 - val_loss: 0.6962 - val_acc: 0.7544\n",
            "Epoch 248/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.6154 - acc: 0.7825 - val_loss: 0.7407 - val_acc: 0.7367\n",
            "Epoch 249/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.6040 - acc: 0.7852 - val_loss: 0.6979 - val_acc: 0.7514\n",
            "Epoch 250/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.6032 - acc: 0.7903 - val_loss: 0.6808 - val_acc: 0.7648\n",
            "Epoch 251/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.5883 - acc: 0.7897 - val_loss: 0.7029 - val_acc: 0.7593\n",
            "Epoch 252/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.5874 - acc: 0.7931 - val_loss: 0.6879 - val_acc: 0.7514\n",
            "Epoch 253/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.5928 - acc: 0.7831 - val_loss: 0.6932 - val_acc: 0.7532\n",
            "Epoch 254/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.5842 - acc: 0.7906 - val_loss: 0.6755 - val_acc: 0.7612\n",
            "Epoch 255/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.5891 - acc: 0.7922 - val_loss: 0.6949 - val_acc: 0.7563\n",
            "Epoch 256/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.5887 - acc: 0.7816 - val_loss: 0.7017 - val_acc: 0.7465\n",
            "Epoch 257/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.5854 - acc: 0.7931 - val_loss: 0.7043 - val_acc: 0.7514\n",
            "Epoch 258/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.5834 - acc: 0.7867 - val_loss: 0.6912 - val_acc: 0.7569\n",
            "Epoch 259/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.5905 - acc: 0.7852 - val_loss: 0.7253 - val_acc: 0.7379\n",
            "Epoch 260/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.5891 - acc: 0.7922 - val_loss: 0.6684 - val_acc: 0.7722\n",
            "Epoch 261/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.5769 - acc: 0.7934 - val_loss: 0.6798 - val_acc: 0.7679\n",
            "Epoch 262/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.5802 - acc: 0.7906 - val_loss: 0.6844 - val_acc: 0.7593\n",
            "Epoch 263/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.5895 - acc: 0.7903 - val_loss: 0.6707 - val_acc: 0.7624\n",
            "Epoch 264/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.5941 - acc: 0.7910 - val_loss: 0.6791 - val_acc: 0.7612\n",
            "Epoch 265/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.5797 - acc: 0.7940 - val_loss: 0.6899 - val_acc: 0.7575\n",
            "Epoch 266/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.5650 - acc: 0.8081 - val_loss: 0.6904 - val_acc: 0.7532\n",
            "Epoch 267/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.5736 - acc: 0.7952 - val_loss: 0.6621 - val_acc: 0.7679\n",
            "Epoch 268/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.5775 - acc: 0.7931 - val_loss: 0.6984 - val_acc: 0.7477\n",
            "Epoch 269/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.5822 - acc: 0.7913 - val_loss: 0.6833 - val_acc: 0.7606\n",
            "Epoch 270/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.5727 - acc: 0.7934 - val_loss: 0.6736 - val_acc: 0.7648\n",
            "Epoch 271/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.5645 - acc: 0.8012 - val_loss: 0.6597 - val_acc: 0.7691\n",
            "Epoch 272/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.5671 - acc: 0.7961 - val_loss: 0.6807 - val_acc: 0.7593\n",
            "Epoch 273/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.5616 - acc: 0.8009 - val_loss: 0.6461 - val_acc: 0.7881\n",
            "Epoch 274/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.5689 - acc: 0.7934 - val_loss: 0.6808 - val_acc: 0.7642\n",
            "Epoch 275/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.5641 - acc: 0.8033 - val_loss: 0.6665 - val_acc: 0.7636\n",
            "Epoch 276/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.5566 - acc: 0.8039 - val_loss: 0.6614 - val_acc: 0.7734\n",
            "Epoch 277/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.5773 - acc: 0.7928 - val_loss: 0.6650 - val_acc: 0.7624\n",
            "Epoch 278/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.5522 - acc: 0.8063 - val_loss: 0.6598 - val_acc: 0.7691\n",
            "Epoch 279/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.5628 - acc: 0.8036 - val_loss: 0.6762 - val_acc: 0.7593\n",
            "Epoch 280/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.5529 - acc: 0.8121 - val_loss: 0.6686 - val_acc: 0.7734\n",
            "Epoch 281/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.5623 - acc: 0.7928 - val_loss: 0.6606 - val_acc: 0.7728\n",
            "Epoch 282/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.5587 - acc: 0.7988 - val_loss: 0.6694 - val_acc: 0.7563\n",
            "Epoch 283/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.5600 - acc: 0.8012 - val_loss: 0.6666 - val_acc: 0.7673\n",
            "Epoch 284/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.5525 - acc: 0.8015 - val_loss: 0.6439 - val_acc: 0.7740\n",
            "Epoch 285/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.5392 - acc: 0.8118 - val_loss: 0.6544 - val_acc: 0.7722\n",
            "Epoch 286/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.5462 - acc: 0.8087 - val_loss: 0.6639 - val_acc: 0.7728\n",
            "Epoch 287/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.5442 - acc: 0.8097 - val_loss: 0.6400 - val_acc: 0.7746\n",
            "Epoch 288/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.5415 - acc: 0.8121 - val_loss: 0.6384 - val_acc: 0.7759\n",
            "Epoch 289/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.5396 - acc: 0.8112 - val_loss: 0.6496 - val_acc: 0.7783\n",
            "Epoch 290/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.5458 - acc: 0.8100 - val_loss: 0.6740 - val_acc: 0.7642\n",
            "Epoch 291/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.5424 - acc: 0.8063 - val_loss: 0.6440 - val_acc: 0.7795\n",
            "Epoch 292/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.5286 - acc: 0.8090 - val_loss: 0.6529 - val_acc: 0.7802\n",
            "Epoch 293/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.5348 - acc: 0.8072 - val_loss: 0.6276 - val_acc: 0.7789\n",
            "Epoch 294/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.5312 - acc: 0.8160 - val_loss: 0.6369 - val_acc: 0.7851\n",
            "Epoch 295/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.5348 - acc: 0.8063 - val_loss: 0.6482 - val_acc: 0.7771\n",
            "Epoch 296/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.5221 - acc: 0.8184 - val_loss: 0.6482 - val_acc: 0.7808\n",
            "Epoch 297/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.5257 - acc: 0.8175 - val_loss: 0.6265 - val_acc: 0.7906\n",
            "Epoch 298/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.5192 - acc: 0.8205 - val_loss: 0.6651 - val_acc: 0.7722\n",
            "Epoch 299/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.5230 - acc: 0.8217 - val_loss: 0.6249 - val_acc: 0.7949\n",
            "Epoch 300/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.5241 - acc: 0.8202 - val_loss: 0.6295 - val_acc: 0.7869\n",
            "Epoch 301/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.5287 - acc: 0.8229 - val_loss: 0.6316 - val_acc: 0.7869\n",
            "Epoch 302/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.5200 - acc: 0.8241 - val_loss: 0.6172 - val_acc: 0.7900\n",
            "Epoch 303/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.5139 - acc: 0.8208 - val_loss: 0.6304 - val_acc: 0.7820\n",
            "Epoch 304/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.5259 - acc: 0.8127 - val_loss: 0.6217 - val_acc: 0.7832\n",
            "Epoch 305/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.5136 - acc: 0.8181 - val_loss: 0.6397 - val_acc: 0.7740\n",
            "Epoch 306/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.5130 - acc: 0.8193 - val_loss: 0.6516 - val_acc: 0.7716\n",
            "Epoch 307/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.5218 - acc: 0.8115 - val_loss: 0.6426 - val_acc: 0.7734\n",
            "Epoch 308/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.4891 - acc: 0.8275 - val_loss: 0.6386 - val_acc: 0.7826\n",
            "Epoch 309/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.5169 - acc: 0.8196 - val_loss: 0.6241 - val_acc: 0.7863\n",
            "Epoch 310/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.5071 - acc: 0.8154 - val_loss: 0.6168 - val_acc: 0.8004\n",
            "Epoch 311/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.5077 - acc: 0.8190 - val_loss: 0.6465 - val_acc: 0.7612\n",
            "Epoch 312/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.5108 - acc: 0.8211 - val_loss: 0.6514 - val_acc: 0.7746\n",
            "Epoch 313/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.5045 - acc: 0.8256 - val_loss: 0.6263 - val_acc: 0.7900\n",
            "Epoch 314/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.5052 - acc: 0.8268 - val_loss: 0.6236 - val_acc: 0.7746\n",
            "Epoch 315/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.4954 - acc: 0.8293 - val_loss: 0.6152 - val_acc: 0.7949\n",
            "Epoch 316/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.5009 - acc: 0.8244 - val_loss: 0.6062 - val_acc: 0.7985\n",
            "Epoch 317/1000\n",
            "3315/3315 [==============================] - 2s 521us/step - loss: 0.4862 - acc: 0.8293 - val_loss: 0.6173 - val_acc: 0.7918\n",
            "Epoch 318/1000\n",
            "3315/3315 [==============================] - 2s 522us/step - loss: 0.4935 - acc: 0.8344 - val_loss: 0.6212 - val_acc: 0.7973\n",
            "Epoch 319/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.4982 - acc: 0.8281 - val_loss: 0.6147 - val_acc: 0.7887\n",
            "Epoch 320/1000\n",
            "3315/3315 [==============================] - 2s 510us/step - loss: 0.5031 - acc: 0.8220 - val_loss: 0.6099 - val_acc: 0.7881\n",
            "Epoch 321/1000\n",
            "3315/3315 [==============================] - 2s 521us/step - loss: 0.4943 - acc: 0.8308 - val_loss: 0.6283 - val_acc: 0.7924\n",
            "Epoch 322/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.4889 - acc: 0.8268 - val_loss: 0.6188 - val_acc: 0.7912\n",
            "Epoch 323/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.4778 - acc: 0.8305 - val_loss: 0.6181 - val_acc: 0.7912\n",
            "Epoch 324/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.4944 - acc: 0.8244 - val_loss: 0.6205 - val_acc: 0.7863\n",
            "Epoch 325/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.4921 - acc: 0.8278 - val_loss: 0.6077 - val_acc: 0.7985\n",
            "Epoch 326/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.4951 - acc: 0.8268 - val_loss: 0.6021 - val_acc: 0.8028\n",
            "Epoch 327/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.4807 - acc: 0.8317 - val_loss: 0.6171 - val_acc: 0.7936\n",
            "Epoch 328/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.4993 - acc: 0.8202 - val_loss: 0.5950 - val_acc: 0.8040\n",
            "Epoch 329/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.4652 - acc: 0.8335 - val_loss: 0.6026 - val_acc: 0.7973\n",
            "Epoch 330/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.4736 - acc: 0.8335 - val_loss: 0.6009 - val_acc: 0.7998\n",
            "Epoch 331/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.4728 - acc: 0.8293 - val_loss: 0.5883 - val_acc: 0.8065\n",
            "Epoch 332/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.4915 - acc: 0.8235 - val_loss: 0.6424 - val_acc: 0.7814\n",
            "Epoch 333/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.4650 - acc: 0.8425 - val_loss: 0.5838 - val_acc: 0.8132\n",
            "Epoch 334/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.4718 - acc: 0.8293 - val_loss: 0.5806 - val_acc: 0.8096\n",
            "Epoch 335/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.4595 - acc: 0.8401 - val_loss: 0.5879 - val_acc: 0.8059\n",
            "Epoch 336/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.4684 - acc: 0.8353 - val_loss: 0.6016 - val_acc: 0.8071\n",
            "Epoch 337/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.4813 - acc: 0.8253 - val_loss: 0.5885 - val_acc: 0.8096\n",
            "Epoch 338/1000\n",
            "3315/3315 [==============================] - 2s 458us/step - loss: 0.4669 - acc: 0.8395 - val_loss: 0.5725 - val_acc: 0.8132\n",
            "Epoch 339/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.4584 - acc: 0.8422 - val_loss: 0.6084 - val_acc: 0.8028\n",
            "Epoch 340/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.4599 - acc: 0.8456 - val_loss: 0.6233 - val_acc: 0.7857\n",
            "Epoch 341/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.4675 - acc: 0.8374 - val_loss: 0.5900 - val_acc: 0.8004\n",
            "Epoch 342/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.4616 - acc: 0.8302 - val_loss: 0.5792 - val_acc: 0.7967\n",
            "Epoch 343/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.4555 - acc: 0.8474 - val_loss: 0.5789 - val_acc: 0.7991\n",
            "Epoch 344/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.4602 - acc: 0.8440 - val_loss: 0.5813 - val_acc: 0.8077\n",
            "Epoch 345/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.4675 - acc: 0.8335 - val_loss: 0.6095 - val_acc: 0.7930\n",
            "Epoch 346/1000\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.4539 - acc: 0.8456 - val_loss: 0.5784 - val_acc: 0.8040\n",
            "Epoch 347/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.4602 - acc: 0.8374 - val_loss: 0.5738 - val_acc: 0.8151\n",
            "Epoch 348/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.4490 - acc: 0.8495 - val_loss: 0.5812 - val_acc: 0.8040\n",
            "Epoch 349/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.4599 - acc: 0.8416 - val_loss: 0.5903 - val_acc: 0.8028\n",
            "Epoch 350/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.4419 - acc: 0.8452 - val_loss: 0.5728 - val_acc: 0.8114\n",
            "Epoch 351/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.4440 - acc: 0.8449 - val_loss: 0.5771 - val_acc: 0.8065\n",
            "Epoch 352/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.4531 - acc: 0.8431 - val_loss: 0.5831 - val_acc: 0.8053\n",
            "Epoch 353/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.4485 - acc: 0.8416 - val_loss: 0.5692 - val_acc: 0.8053\n",
            "Epoch 354/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.4562 - acc: 0.8347 - val_loss: 0.5785 - val_acc: 0.8034\n",
            "Epoch 355/1000\n",
            "3315/3315 [==============================] - 2s 493us/step - loss: 0.4386 - acc: 0.8410 - val_loss: 0.5640 - val_acc: 0.8163\n",
            "Epoch 356/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.4417 - acc: 0.8474 - val_loss: 0.5647 - val_acc: 0.8145\n",
            "Epoch 357/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.4481 - acc: 0.8431 - val_loss: 0.5841 - val_acc: 0.8022\n",
            "Epoch 358/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.4344 - acc: 0.8540 - val_loss: 0.5710 - val_acc: 0.8138\n",
            "Epoch 359/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.4431 - acc: 0.8474 - val_loss: 0.5590 - val_acc: 0.8218\n",
            "Epoch 360/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.4436 - acc: 0.8480 - val_loss: 0.5759 - val_acc: 0.8138\n",
            "Epoch 361/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.4324 - acc: 0.8501 - val_loss: 0.6363 - val_acc: 0.7765\n",
            "Epoch 362/1000\n",
            "3315/3315 [==============================] - 2s 491us/step - loss: 0.4357 - acc: 0.8456 - val_loss: 0.5632 - val_acc: 0.8194\n",
            "Epoch 363/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.4386 - acc: 0.8516 - val_loss: 0.5673 - val_acc: 0.8065\n",
            "Epoch 364/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.4252 - acc: 0.8486 - val_loss: 0.5518 - val_acc: 0.8187\n",
            "Epoch 365/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.4339 - acc: 0.8456 - val_loss: 0.5627 - val_acc: 0.8065\n",
            "Epoch 366/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.4356 - acc: 0.8452 - val_loss: 0.5639 - val_acc: 0.8108\n",
            "Epoch 367/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.4409 - acc: 0.8449 - val_loss: 0.5534 - val_acc: 0.8151\n",
            "Epoch 368/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.4291 - acc: 0.8480 - val_loss: 0.5790 - val_acc: 0.8151\n",
            "Epoch 369/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.4360 - acc: 0.8462 - val_loss: 0.5778 - val_acc: 0.8114\n",
            "Epoch 370/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.4309 - acc: 0.8495 - val_loss: 0.5436 - val_acc: 0.8340\n",
            "Epoch 371/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.4364 - acc: 0.8465 - val_loss: 0.5539 - val_acc: 0.8175\n",
            "Epoch 372/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.4191 - acc: 0.8612 - val_loss: 0.5396 - val_acc: 0.8279\n",
            "Epoch 373/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.4181 - acc: 0.8513 - val_loss: 0.5517 - val_acc: 0.8187\n",
            "Epoch 374/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.4191 - acc: 0.8588 - val_loss: 0.5763 - val_acc: 0.8059\n",
            "Epoch 375/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.4197 - acc: 0.8471 - val_loss: 0.5669 - val_acc: 0.8040\n",
            "Epoch 376/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.4246 - acc: 0.8510 - val_loss: 0.5769 - val_acc: 0.8132\n",
            "Epoch 377/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.4202 - acc: 0.8579 - val_loss: 0.5379 - val_acc: 0.8291\n",
            "Epoch 378/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.4196 - acc: 0.8501 - val_loss: 0.5805 - val_acc: 0.8034\n",
            "Epoch 379/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.4187 - acc: 0.8504 - val_loss: 0.5445 - val_acc: 0.8267\n",
            "Epoch 380/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.4215 - acc: 0.8546 - val_loss: 0.5478 - val_acc: 0.8181\n",
            "Epoch 381/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.4082 - acc: 0.8606 - val_loss: 0.5365 - val_acc: 0.8218\n",
            "Epoch 382/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.4184 - acc: 0.8516 - val_loss: 0.5379 - val_acc: 0.8224\n",
            "Epoch 383/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.4214 - acc: 0.8549 - val_loss: 0.6328 - val_acc: 0.7820\n",
            "Epoch 384/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.4148 - acc: 0.8570 - val_loss: 0.5352 - val_acc: 0.8218\n",
            "Epoch 385/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.4160 - acc: 0.8570 - val_loss: 0.5570 - val_acc: 0.8138\n",
            "Epoch 386/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.4245 - acc: 0.8501 - val_loss: 0.5381 - val_acc: 0.8249\n",
            "Epoch 387/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.4156 - acc: 0.8576 - val_loss: 0.5575 - val_acc: 0.8145\n",
            "Epoch 388/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.4194 - acc: 0.8579 - val_loss: 0.5451 - val_acc: 0.8255\n",
            "Epoch 389/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3972 - acc: 0.8655 - val_loss: 0.5342 - val_acc: 0.8261\n",
            "Epoch 390/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.4020 - acc: 0.8600 - val_loss: 0.5429 - val_acc: 0.8218\n",
            "Epoch 391/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.3954 - acc: 0.8637 - val_loss: 0.5406 - val_acc: 0.8285\n",
            "Epoch 392/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.4142 - acc: 0.8594 - val_loss: 0.5404 - val_acc: 0.8304\n",
            "Epoch 393/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.4088 - acc: 0.8573 - val_loss: 0.5429 - val_acc: 0.8236\n",
            "Epoch 394/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.4094 - acc: 0.8564 - val_loss: 0.5381 - val_acc: 0.8194\n",
            "Epoch 395/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.4089 - acc: 0.8525 - val_loss: 0.5462 - val_acc: 0.8187\n",
            "Epoch 396/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.4022 - acc: 0.8649 - val_loss: 0.5421 - val_acc: 0.8218\n",
            "Epoch 397/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.3961 - acc: 0.8685 - val_loss: 0.5371 - val_acc: 0.8181\n",
            "Epoch 398/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3980 - acc: 0.8606 - val_loss: 0.5432 - val_acc: 0.8181\n",
            "Epoch 399/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.4068 - acc: 0.8567 - val_loss: 0.5386 - val_acc: 0.8132\n",
            "Epoch 400/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.4070 - acc: 0.8588 - val_loss: 0.5472 - val_acc: 0.8285\n",
            "Epoch 401/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.3850 - acc: 0.8640 - val_loss: 0.5323 - val_acc: 0.8298\n",
            "Epoch 402/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3944 - acc: 0.8679 - val_loss: 0.5194 - val_acc: 0.8340\n",
            "Epoch 403/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.4140 - acc: 0.8585 - val_loss: 0.5196 - val_acc: 0.8334\n",
            "Epoch 404/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.4027 - acc: 0.8585 - val_loss: 0.5359 - val_acc: 0.8206\n",
            "Epoch 405/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3817 - acc: 0.8624 - val_loss: 0.5215 - val_acc: 0.8328\n",
            "Epoch 406/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3901 - acc: 0.8640 - val_loss: 0.5220 - val_acc: 0.8340\n",
            "Epoch 407/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.3923 - acc: 0.8606 - val_loss: 0.5279 - val_acc: 0.8291\n",
            "Epoch 408/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3879 - acc: 0.8679 - val_loss: 0.5185 - val_acc: 0.8334\n",
            "Epoch 409/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.3904 - acc: 0.8682 - val_loss: 0.5561 - val_acc: 0.8096\n",
            "Epoch 410/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.3998 - acc: 0.8564 - val_loss: 0.5272 - val_acc: 0.8242\n",
            "Epoch 411/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.3756 - acc: 0.8703 - val_loss: 0.5366 - val_acc: 0.8242\n",
            "Epoch 412/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3793 - acc: 0.8652 - val_loss: 0.5171 - val_acc: 0.8224\n",
            "Epoch 413/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3779 - acc: 0.8712 - val_loss: 0.5304 - val_acc: 0.8261\n",
            "Epoch 414/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.3899 - acc: 0.8673 - val_loss: 0.5350 - val_acc: 0.8316\n",
            "Epoch 415/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3851 - acc: 0.8682 - val_loss: 0.5505 - val_acc: 0.8108\n",
            "Epoch 416/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3990 - acc: 0.8555 - val_loss: 0.5220 - val_acc: 0.8249\n",
            "Epoch 417/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.4024 - acc: 0.8558 - val_loss: 0.5340 - val_acc: 0.8187\n",
            "Epoch 418/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3809 - acc: 0.8742 - val_loss: 0.5187 - val_acc: 0.8206\n",
            "Epoch 419/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.3834 - acc: 0.8676 - val_loss: 0.5154 - val_acc: 0.8383\n",
            "Epoch 420/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.3782 - acc: 0.8676 - val_loss: 0.5116 - val_acc: 0.8322\n",
            "Epoch 421/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3861 - acc: 0.8664 - val_loss: 0.5211 - val_acc: 0.8365\n",
            "Epoch 422/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3721 - acc: 0.8670 - val_loss: 0.5129 - val_acc: 0.8353\n",
            "Epoch 423/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3747 - acc: 0.8724 - val_loss: 0.4987 - val_acc: 0.8389\n",
            "Epoch 424/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.3669 - acc: 0.8706 - val_loss: 0.5148 - val_acc: 0.8267\n",
            "Epoch 425/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3775 - acc: 0.8685 - val_loss: 0.5209 - val_acc: 0.8383\n",
            "Epoch 426/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3714 - acc: 0.8754 - val_loss: 0.5157 - val_acc: 0.8353\n",
            "Epoch 427/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3684 - acc: 0.8670 - val_loss: 0.4949 - val_acc: 0.8451\n",
            "Epoch 428/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.3784 - acc: 0.8685 - val_loss: 0.5181 - val_acc: 0.8340\n",
            "Epoch 429/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.3730 - acc: 0.8700 - val_loss: 0.4980 - val_acc: 0.8396\n",
            "Epoch 430/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.3564 - acc: 0.8730 - val_loss: 0.5168 - val_acc: 0.8230\n",
            "Epoch 431/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3678 - acc: 0.8724 - val_loss: 0.5191 - val_acc: 0.8230\n",
            "Epoch 432/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3721 - acc: 0.8718 - val_loss: 0.5352 - val_acc: 0.8187\n",
            "Epoch 433/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3769 - acc: 0.8643 - val_loss: 0.5135 - val_acc: 0.8304\n",
            "Epoch 434/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3774 - acc: 0.8700 - val_loss: 0.5241 - val_acc: 0.8359\n",
            "Epoch 435/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3609 - acc: 0.8742 - val_loss: 0.5029 - val_acc: 0.8377\n",
            "Epoch 436/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3530 - acc: 0.8824 - val_loss: 0.5308 - val_acc: 0.8273\n",
            "Epoch 437/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.3587 - acc: 0.8805 - val_loss: 0.5059 - val_acc: 0.8291\n",
            "Epoch 438/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.3451 - acc: 0.8839 - val_loss: 0.5025 - val_acc: 0.8451\n",
            "Epoch 439/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.3720 - acc: 0.8736 - val_loss: 0.5172 - val_acc: 0.8279\n",
            "Epoch 440/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.3603 - acc: 0.8802 - val_loss: 0.4964 - val_acc: 0.8451\n",
            "Epoch 441/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.3658 - acc: 0.8757 - val_loss: 0.5172 - val_acc: 0.8267\n",
            "Epoch 442/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.3632 - acc: 0.8790 - val_loss: 0.5121 - val_acc: 0.8261\n",
            "Epoch 443/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3549 - acc: 0.8784 - val_loss: 0.5038 - val_acc: 0.8383\n",
            "Epoch 444/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3535 - acc: 0.8754 - val_loss: 0.4924 - val_acc: 0.8359\n",
            "Epoch 445/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.3466 - acc: 0.8836 - val_loss: 0.5118 - val_acc: 0.8298\n",
            "Epoch 446/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3488 - acc: 0.8796 - val_loss: 0.5094 - val_acc: 0.8365\n",
            "Epoch 447/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3556 - acc: 0.8817 - val_loss: 0.4980 - val_acc: 0.8408\n",
            "Epoch 448/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3394 - acc: 0.8869 - val_loss: 0.5357 - val_acc: 0.8285\n",
            "Epoch 449/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3458 - acc: 0.8836 - val_loss: 0.5024 - val_acc: 0.8353\n",
            "Epoch 450/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.3450 - acc: 0.8839 - val_loss: 0.4914 - val_acc: 0.8414\n",
            "Epoch 451/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.3520 - acc: 0.8811 - val_loss: 0.5096 - val_acc: 0.8261\n",
            "Epoch 452/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3609 - acc: 0.8673 - val_loss: 0.4834 - val_acc: 0.8438\n",
            "Epoch 453/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3497 - acc: 0.8793 - val_loss: 0.5035 - val_acc: 0.8340\n",
            "Epoch 454/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.3507 - acc: 0.8751 - val_loss: 0.5131 - val_acc: 0.8328\n",
            "Epoch 455/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.3346 - acc: 0.8799 - val_loss: 0.4928 - val_acc: 0.8438\n",
            "Epoch 456/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.3563 - acc: 0.8748 - val_loss: 0.4931 - val_acc: 0.8481\n",
            "Epoch 457/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.3505 - acc: 0.8854 - val_loss: 0.4939 - val_acc: 0.8426\n",
            "Epoch 458/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.3493 - acc: 0.8802 - val_loss: 0.4847 - val_acc: 0.8457\n",
            "Epoch 459/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3385 - acc: 0.8923 - val_loss: 0.4885 - val_acc: 0.8371\n",
            "Epoch 460/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3477 - acc: 0.8817 - val_loss: 0.4815 - val_acc: 0.8506\n",
            "Epoch 461/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3362 - acc: 0.8845 - val_loss: 0.4790 - val_acc: 0.8487\n",
            "Epoch 462/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.3370 - acc: 0.8814 - val_loss: 0.4796 - val_acc: 0.8438\n",
            "Epoch 463/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3458 - acc: 0.8769 - val_loss: 0.4956 - val_acc: 0.8451\n",
            "Epoch 464/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.3377 - acc: 0.8811 - val_loss: 0.4750 - val_acc: 0.8469\n",
            "Epoch 465/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.3417 - acc: 0.8784 - val_loss: 0.4827 - val_acc: 0.8414\n",
            "Epoch 466/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.3549 - acc: 0.8784 - val_loss: 0.5015 - val_acc: 0.8365\n",
            "Epoch 467/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.3196 - acc: 0.8899 - val_loss: 0.4975 - val_acc: 0.8402\n",
            "Epoch 468/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3441 - acc: 0.8763 - val_loss: 0.4919 - val_acc: 0.8420\n",
            "Epoch 469/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.3321 - acc: 0.8827 - val_loss: 0.4868 - val_acc: 0.8494\n",
            "Epoch 470/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.3461 - acc: 0.8751 - val_loss: 0.4892 - val_acc: 0.8359\n",
            "Epoch 471/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3274 - acc: 0.8950 - val_loss: 0.4972 - val_acc: 0.8340\n",
            "Epoch 472/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.3339 - acc: 0.8875 - val_loss: 0.4819 - val_acc: 0.8469\n",
            "Epoch 473/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.3408 - acc: 0.8821 - val_loss: 0.4735 - val_acc: 0.8481\n",
            "Epoch 474/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.3443 - acc: 0.8863 - val_loss: 0.4885 - val_acc: 0.8420\n",
            "Epoch 475/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3338 - acc: 0.8842 - val_loss: 0.4738 - val_acc: 0.8592\n",
            "Epoch 476/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3364 - acc: 0.8830 - val_loss: 0.4959 - val_acc: 0.8420\n",
            "Epoch 477/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.3258 - acc: 0.8908 - val_loss: 0.4799 - val_acc: 0.8426\n",
            "Epoch 478/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3250 - acc: 0.8869 - val_loss: 0.5031 - val_acc: 0.8383\n",
            "Epoch 479/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3380 - acc: 0.8808 - val_loss: 0.4930 - val_acc: 0.8494\n",
            "Epoch 480/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.3354 - acc: 0.8878 - val_loss: 0.4836 - val_acc: 0.8500\n",
            "Epoch 481/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.3327 - acc: 0.8854 - val_loss: 0.4885 - val_acc: 0.8396\n",
            "Epoch 482/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.3220 - acc: 0.8920 - val_loss: 0.4691 - val_acc: 0.8604\n",
            "Epoch 483/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.3373 - acc: 0.8802 - val_loss: 0.4888 - val_acc: 0.8414\n",
            "Epoch 484/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3250 - acc: 0.8824 - val_loss: 0.4772 - val_acc: 0.8377\n",
            "Epoch 485/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3196 - acc: 0.8875 - val_loss: 0.4854 - val_acc: 0.8445\n",
            "Epoch 486/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3089 - acc: 0.8920 - val_loss: 0.4813 - val_acc: 0.8506\n",
            "Epoch 487/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.3263 - acc: 0.8875 - val_loss: 0.4803 - val_acc: 0.8463\n",
            "Epoch 488/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.3182 - acc: 0.8953 - val_loss: 0.5013 - val_acc: 0.8408\n",
            "Epoch 489/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.3414 - acc: 0.8796 - val_loss: 0.4643 - val_acc: 0.8573\n",
            "Epoch 490/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.3159 - acc: 0.8944 - val_loss: 0.4725 - val_acc: 0.8500\n",
            "Epoch 491/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3186 - acc: 0.8935 - val_loss: 0.5227 - val_acc: 0.8291\n",
            "Epoch 492/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.3210 - acc: 0.8923 - val_loss: 0.4853 - val_acc: 0.8359\n",
            "Epoch 493/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.3112 - acc: 0.8944 - val_loss: 0.4744 - val_acc: 0.8475\n",
            "Epoch 494/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3205 - acc: 0.8872 - val_loss: 0.4557 - val_acc: 0.8555\n",
            "Epoch 495/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3119 - acc: 0.8920 - val_loss: 0.4717 - val_acc: 0.8592\n",
            "Epoch 496/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3096 - acc: 0.8920 - val_loss: 0.4595 - val_acc: 0.8567\n",
            "Epoch 497/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.3187 - acc: 0.8866 - val_loss: 0.4471 - val_acc: 0.8604\n",
            "Epoch 498/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.3215 - acc: 0.8887 - val_loss: 0.4736 - val_acc: 0.8549\n",
            "Epoch 499/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.3204 - acc: 0.8914 - val_loss: 0.4694 - val_acc: 0.8543\n",
            "Epoch 500/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.3218 - acc: 0.8890 - val_loss: 0.4803 - val_acc: 0.8500\n",
            "Epoch 501/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3304 - acc: 0.8799 - val_loss: 0.4512 - val_acc: 0.8506\n",
            "Epoch 502/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3183 - acc: 0.8884 - val_loss: 0.4538 - val_acc: 0.8549\n",
            "Epoch 503/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3270 - acc: 0.8799 - val_loss: 0.4417 - val_acc: 0.8665\n",
            "Epoch 504/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.3039 - acc: 0.8956 - val_loss: 0.4636 - val_acc: 0.8543\n",
            "Epoch 505/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.3098 - acc: 0.8953 - val_loss: 0.4490 - val_acc: 0.8659\n",
            "Epoch 506/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3106 - acc: 0.8968 - val_loss: 0.4686 - val_acc: 0.8438\n",
            "Epoch 507/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3220 - acc: 0.8869 - val_loss: 0.4667 - val_acc: 0.8536\n",
            "Epoch 508/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.3097 - acc: 0.8896 - val_loss: 0.4720 - val_acc: 0.8512\n",
            "Epoch 509/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.3024 - acc: 0.8944 - val_loss: 0.4588 - val_acc: 0.8506\n",
            "Epoch 510/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.3011 - acc: 0.8974 - val_loss: 0.4461 - val_acc: 0.8622\n",
            "Epoch 511/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3198 - acc: 0.8863 - val_loss: 0.4886 - val_acc: 0.8487\n",
            "Epoch 512/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.3111 - acc: 0.8977 - val_loss: 0.4551 - val_acc: 0.8549\n",
            "Epoch 513/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.3062 - acc: 0.8986 - val_loss: 0.4599 - val_acc: 0.8610\n",
            "Epoch 514/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.3086 - acc: 0.8944 - val_loss: 0.4549 - val_acc: 0.8647\n",
            "Epoch 515/1000\n",
            "3315/3315 [==============================] - 2s 512us/step - loss: 0.2993 - acc: 0.8968 - val_loss: 0.4619 - val_acc: 0.8610\n",
            "Epoch 516/1000\n",
            "3315/3315 [==============================] - 2s 514us/step - loss: 0.2986 - acc: 0.9017 - val_loss: 0.4499 - val_acc: 0.8561\n",
            "Epoch 517/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 0.3148 - acc: 0.8887 - val_loss: 0.4537 - val_acc: 0.8610\n",
            "Epoch 518/1000\n",
            "3315/3315 [==============================] - 2s 516us/step - loss: 0.3146 - acc: 0.8965 - val_loss: 0.4743 - val_acc: 0.8512\n",
            "Epoch 519/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.3035 - acc: 0.8944 - val_loss: 0.4542 - val_acc: 0.8641\n",
            "Epoch 520/1000\n",
            "3315/3315 [==============================] - 2s 500us/step - loss: 0.2863 - acc: 0.9047 - val_loss: 0.4626 - val_acc: 0.8506\n",
            "Epoch 521/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.3128 - acc: 0.8947 - val_loss: 0.4453 - val_acc: 0.8659\n",
            "Epoch 522/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.2938 - acc: 0.8968 - val_loss: 0.4586 - val_acc: 0.8598\n",
            "Epoch 523/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.3041 - acc: 0.8968 - val_loss: 0.4680 - val_acc: 0.8438\n",
            "Epoch 524/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.3011 - acc: 0.9005 - val_loss: 0.4542 - val_acc: 0.8543\n",
            "Epoch 525/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.2925 - acc: 0.9011 - val_loss: 0.4696 - val_acc: 0.8475\n",
            "Epoch 526/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2920 - acc: 0.9038 - val_loss: 0.4275 - val_acc: 0.8653\n",
            "Epoch 527/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2983 - acc: 0.9038 - val_loss: 0.4462 - val_acc: 0.8610\n",
            "Epoch 528/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2982 - acc: 0.8986 - val_loss: 0.4382 - val_acc: 0.8641\n",
            "Epoch 529/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2873 - acc: 0.8995 - val_loss: 0.4599 - val_acc: 0.8506\n",
            "Epoch 530/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2848 - acc: 0.9026 - val_loss: 0.4566 - val_acc: 0.8708\n",
            "Epoch 531/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2878 - acc: 0.9026 - val_loss: 0.4466 - val_acc: 0.8653\n",
            "Epoch 532/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2945 - acc: 0.8989 - val_loss: 0.4340 - val_acc: 0.8714\n",
            "Epoch 533/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2883 - acc: 0.9041 - val_loss: 0.4387 - val_acc: 0.8610\n",
            "Epoch 534/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2784 - acc: 0.9074 - val_loss: 0.4666 - val_acc: 0.8487\n",
            "Epoch 535/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.2859 - acc: 0.8962 - val_loss: 0.4472 - val_acc: 0.8616\n",
            "Epoch 536/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2937 - acc: 0.8950 - val_loss: 0.4376 - val_acc: 0.8561\n",
            "Epoch 537/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2812 - acc: 0.9062 - val_loss: 0.4390 - val_acc: 0.8739\n",
            "Epoch 538/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.2859 - acc: 0.9056 - val_loss: 0.4432 - val_acc: 0.8573\n",
            "Epoch 539/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.2950 - acc: 0.9002 - val_loss: 0.4449 - val_acc: 0.8592\n",
            "Epoch 540/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.2875 - acc: 0.9020 - val_loss: 0.4490 - val_acc: 0.8616\n",
            "Epoch 541/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.2885 - acc: 0.8983 - val_loss: 0.4529 - val_acc: 0.8585\n",
            "Epoch 542/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2897 - acc: 0.8971 - val_loss: 0.4357 - val_acc: 0.8598\n",
            "Epoch 543/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2797 - acc: 0.9029 - val_loss: 0.4391 - val_acc: 0.8683\n",
            "Epoch 544/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.2900 - acc: 0.8992 - val_loss: 0.4553 - val_acc: 0.8665\n",
            "Epoch 545/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2729 - acc: 0.9083 - val_loss: 0.4342 - val_acc: 0.8671\n",
            "Epoch 546/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.2891 - acc: 0.8959 - val_loss: 0.4268 - val_acc: 0.8690\n",
            "Epoch 547/1000\n",
            "3315/3315 [==============================] - 2s 526us/step - loss: 0.2933 - acc: 0.9056 - val_loss: 0.4294 - val_acc: 0.8683\n",
            "Epoch 548/1000\n",
            "3315/3315 [==============================] - 2s 526us/step - loss: 0.2927 - acc: 0.9005 - val_loss: 0.4269 - val_acc: 0.8647\n",
            "Epoch 549/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.2792 - acc: 0.9047 - val_loss: 0.4646 - val_acc: 0.8506\n",
            "Epoch 550/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2717 - acc: 0.9032 - val_loss: 0.4372 - val_acc: 0.8628\n",
            "Epoch 551/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2806 - acc: 0.9032 - val_loss: 0.4274 - val_acc: 0.8702\n",
            "Epoch 552/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2961 - acc: 0.8953 - val_loss: 0.4392 - val_acc: 0.8775\n",
            "Epoch 553/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.2780 - acc: 0.9035 - val_loss: 0.4292 - val_acc: 0.8696\n",
            "Epoch 554/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2909 - acc: 0.8995 - val_loss: 0.4387 - val_acc: 0.8641\n",
            "Epoch 555/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.2802 - acc: 0.9071 - val_loss: 0.4483 - val_acc: 0.8683\n",
            "Epoch 556/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2738 - acc: 0.9080 - val_loss: 0.4463 - val_acc: 0.8543\n",
            "Epoch 557/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2892 - acc: 0.8983 - val_loss: 0.4352 - val_acc: 0.8653\n",
            "Epoch 558/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.2909 - acc: 0.9035 - val_loss: 0.4374 - val_acc: 0.8659\n",
            "Epoch 559/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.2700 - acc: 0.9056 - val_loss: 0.4287 - val_acc: 0.8665\n",
            "Epoch 560/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2738 - acc: 0.9086 - val_loss: 0.4215 - val_acc: 0.8683\n",
            "Epoch 561/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.2888 - acc: 0.9041 - val_loss: 0.4269 - val_acc: 0.8745\n",
            "Epoch 562/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2657 - acc: 0.9119 - val_loss: 0.4195 - val_acc: 0.8683\n",
            "Epoch 563/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2769 - acc: 0.9014 - val_loss: 0.4405 - val_acc: 0.8561\n",
            "Epoch 564/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.2772 - acc: 0.9062 - val_loss: 0.4348 - val_acc: 0.8677\n",
            "Epoch 565/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.2726 - acc: 0.9077 - val_loss: 0.4310 - val_acc: 0.8732\n",
            "Epoch 566/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2738 - acc: 0.9089 - val_loss: 0.4425 - val_acc: 0.8628\n",
            "Epoch 567/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2800 - acc: 0.9071 - val_loss: 0.4342 - val_acc: 0.8634\n",
            "Epoch 568/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2831 - acc: 0.9071 - val_loss: 0.4191 - val_acc: 0.8775\n",
            "Epoch 569/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2726 - acc: 0.9059 - val_loss: 0.4189 - val_acc: 0.8720\n",
            "Epoch 570/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2709 - acc: 0.9059 - val_loss: 0.4456 - val_acc: 0.8579\n",
            "Epoch 571/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2728 - acc: 0.9056 - val_loss: 0.4466 - val_acc: 0.8634\n",
            "Epoch 572/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.2652 - acc: 0.9038 - val_loss: 0.4277 - val_acc: 0.8659\n",
            "Epoch 573/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2555 - acc: 0.9140 - val_loss: 0.4268 - val_acc: 0.8751\n",
            "Epoch 574/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2609 - acc: 0.9116 - val_loss: 0.4178 - val_acc: 0.8696\n",
            "Epoch 575/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2761 - acc: 0.9002 - val_loss: 0.4242 - val_acc: 0.8751\n",
            "Epoch 576/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2756 - acc: 0.9059 - val_loss: 0.4321 - val_acc: 0.8677\n",
            "Epoch 577/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2598 - acc: 0.9113 - val_loss: 0.4690 - val_acc: 0.8536\n",
            "Epoch 578/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2766 - acc: 0.9122 - val_loss: 0.4248 - val_acc: 0.8702\n",
            "Epoch 579/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2617 - acc: 0.9071 - val_loss: 0.4428 - val_acc: 0.8732\n",
            "Epoch 580/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.2748 - acc: 0.9080 - val_loss: 0.4273 - val_acc: 0.8634\n",
            "Epoch 581/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2590 - acc: 0.9140 - val_loss: 0.4201 - val_acc: 0.8812\n",
            "Epoch 582/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.2812 - acc: 0.9050 - val_loss: 0.4033 - val_acc: 0.8769\n",
            "Epoch 583/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2543 - acc: 0.9149 - val_loss: 0.4308 - val_acc: 0.8641\n",
            "Epoch 584/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2723 - acc: 0.9035 - val_loss: 0.4267 - val_acc: 0.8622\n",
            "Epoch 585/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2584 - acc: 0.9092 - val_loss: 0.4273 - val_acc: 0.8732\n",
            "Epoch 586/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2716 - acc: 0.9008 - val_loss: 0.4209 - val_acc: 0.8720\n",
            "Epoch 587/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.2458 - acc: 0.9149 - val_loss: 0.4271 - val_acc: 0.8671\n",
            "Epoch 588/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2478 - acc: 0.9149 - val_loss: 0.4171 - val_acc: 0.8757\n",
            "Epoch 589/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2578 - acc: 0.9119 - val_loss: 0.4245 - val_acc: 0.8690\n",
            "Epoch 590/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2677 - acc: 0.9053 - val_loss: 0.4064 - val_acc: 0.8739\n",
            "Epoch 591/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2512 - acc: 0.9128 - val_loss: 0.4172 - val_acc: 0.8757\n",
            "Epoch 592/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2608 - acc: 0.9113 - val_loss: 0.4306 - val_acc: 0.8696\n",
            "Epoch 593/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2646 - acc: 0.9074 - val_loss: 0.4304 - val_acc: 0.8720\n",
            "Epoch 594/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2552 - acc: 0.9143 - val_loss: 0.4085 - val_acc: 0.8763\n",
            "Epoch 595/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2534 - acc: 0.9131 - val_loss: 0.4202 - val_acc: 0.8769\n",
            "Epoch 596/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2487 - acc: 0.9152 - val_loss: 0.4244 - val_acc: 0.8800\n",
            "Epoch 597/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2598 - acc: 0.9134 - val_loss: 0.4364 - val_acc: 0.8659\n",
            "Epoch 598/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2533 - acc: 0.9146 - val_loss: 0.4141 - val_acc: 0.8775\n",
            "Epoch 599/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2348 - acc: 0.9231 - val_loss: 0.4296 - val_acc: 0.8714\n",
            "Epoch 600/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2442 - acc: 0.9186 - val_loss: 0.4321 - val_acc: 0.8573\n",
            "Epoch 601/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2485 - acc: 0.9186 - val_loss: 0.4043 - val_acc: 0.8769\n",
            "Epoch 602/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2432 - acc: 0.9125 - val_loss: 0.4175 - val_acc: 0.8665\n",
            "Epoch 603/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2518 - acc: 0.9176 - val_loss: 0.4035 - val_acc: 0.8781\n",
            "Epoch 604/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2475 - acc: 0.9176 - val_loss: 0.4089 - val_acc: 0.8806\n",
            "Epoch 605/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2557 - acc: 0.9167 - val_loss: 0.4150 - val_acc: 0.8641\n",
            "Epoch 606/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2474 - acc: 0.9186 - val_loss: 0.4086 - val_acc: 0.8726\n",
            "Epoch 607/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2581 - acc: 0.9186 - val_loss: 0.4107 - val_acc: 0.8800\n",
            "Epoch 608/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2608 - acc: 0.9128 - val_loss: 0.4228 - val_acc: 0.8683\n",
            "Epoch 609/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2615 - acc: 0.9092 - val_loss: 0.4169 - val_acc: 0.8788\n",
            "Epoch 610/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2562 - acc: 0.9110 - val_loss: 0.4128 - val_acc: 0.8690\n",
            "Epoch 611/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2572 - acc: 0.9089 - val_loss: 0.4262 - val_acc: 0.8732\n",
            "Epoch 612/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2609 - acc: 0.9107 - val_loss: 0.4067 - val_acc: 0.8806\n",
            "Epoch 613/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2553 - acc: 0.9186 - val_loss: 0.4285 - val_acc: 0.8634\n",
            "Epoch 614/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2447 - acc: 0.9216 - val_loss: 0.4512 - val_acc: 0.8677\n",
            "Epoch 615/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.2445 - acc: 0.9158 - val_loss: 0.4065 - val_acc: 0.8885\n",
            "Epoch 616/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2505 - acc: 0.9161 - val_loss: 0.4286 - val_acc: 0.8683\n",
            "Epoch 617/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2511 - acc: 0.9155 - val_loss: 0.4201 - val_acc: 0.8751\n",
            "Epoch 618/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.2504 - acc: 0.9152 - val_loss: 0.4355 - val_acc: 0.8739\n",
            "Epoch 619/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2400 - acc: 0.9204 - val_loss: 0.4137 - val_acc: 0.8739\n",
            "Epoch 620/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2487 - acc: 0.9176 - val_loss: 0.4271 - val_acc: 0.8690\n",
            "Epoch 621/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2400 - acc: 0.9143 - val_loss: 0.3979 - val_acc: 0.8843\n",
            "Epoch 622/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2460 - acc: 0.9170 - val_loss: 0.4028 - val_acc: 0.8769\n",
            "Epoch 623/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2324 - acc: 0.9234 - val_loss: 0.4196 - val_acc: 0.8720\n",
            "Epoch 624/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.2495 - acc: 0.9173 - val_loss: 0.4096 - val_acc: 0.8739\n",
            "Epoch 625/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.2359 - acc: 0.9240 - val_loss: 0.4291 - val_acc: 0.8647\n",
            "Epoch 626/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.2549 - acc: 0.9113 - val_loss: 0.4176 - val_acc: 0.8671\n",
            "Epoch 627/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2398 - acc: 0.9179 - val_loss: 0.4051 - val_acc: 0.8708\n",
            "Epoch 628/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2430 - acc: 0.9173 - val_loss: 0.4070 - val_acc: 0.8781\n",
            "Epoch 629/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2498 - acc: 0.9164 - val_loss: 0.3904 - val_acc: 0.8824\n",
            "Epoch 630/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.2345 - acc: 0.9216 - val_loss: 0.4097 - val_acc: 0.8745\n",
            "Epoch 631/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2460 - acc: 0.9155 - val_loss: 0.4027 - val_acc: 0.8855\n",
            "Epoch 632/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.2387 - acc: 0.9225 - val_loss: 0.3881 - val_acc: 0.8947\n",
            "Epoch 633/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2348 - acc: 0.9176 - val_loss: 0.4016 - val_acc: 0.8794\n",
            "Epoch 634/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2300 - acc: 0.9234 - val_loss: 0.4040 - val_acc: 0.8843\n",
            "Epoch 635/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2365 - acc: 0.9179 - val_loss: 0.4300 - val_acc: 0.8794\n",
            "Epoch 636/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2247 - acc: 0.9279 - val_loss: 0.4048 - val_acc: 0.8800\n",
            "Epoch 637/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2266 - acc: 0.9216 - val_loss: 0.4321 - val_acc: 0.8714\n",
            "Epoch 638/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2334 - acc: 0.9210 - val_loss: 0.4056 - val_acc: 0.8836\n",
            "Epoch 639/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2265 - acc: 0.9240 - val_loss: 0.4259 - val_acc: 0.8671\n",
            "Epoch 640/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2310 - acc: 0.9237 - val_loss: 0.3984 - val_acc: 0.8812\n",
            "Epoch 641/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2361 - acc: 0.9249 - val_loss: 0.4011 - val_acc: 0.8788\n",
            "Epoch 642/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2253 - acc: 0.9240 - val_loss: 0.3961 - val_acc: 0.8836\n",
            "Epoch 643/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2427 - acc: 0.9125 - val_loss: 0.3945 - val_acc: 0.8971\n",
            "Epoch 644/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.2347 - acc: 0.9207 - val_loss: 0.4064 - val_acc: 0.8745\n",
            "Epoch 645/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2444 - acc: 0.9137 - val_loss: 0.3959 - val_acc: 0.8781\n",
            "Epoch 646/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2221 - acc: 0.9246 - val_loss: 0.3825 - val_acc: 0.8818\n",
            "Epoch 647/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2246 - acc: 0.9270 - val_loss: 0.3986 - val_acc: 0.8836\n",
            "Epoch 648/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.2364 - acc: 0.9173 - val_loss: 0.3876 - val_acc: 0.8904\n",
            "Epoch 649/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2247 - acc: 0.9249 - val_loss: 0.3880 - val_acc: 0.8830\n",
            "Epoch 650/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.2294 - acc: 0.9264 - val_loss: 0.3908 - val_acc: 0.8885\n",
            "Epoch 651/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2339 - acc: 0.9258 - val_loss: 0.3997 - val_acc: 0.8861\n",
            "Epoch 652/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2260 - acc: 0.9237 - val_loss: 0.3952 - val_acc: 0.8830\n",
            "Epoch 653/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.2273 - acc: 0.9261 - val_loss: 0.4019 - val_acc: 0.8775\n",
            "Epoch 654/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2372 - acc: 0.9228 - val_loss: 0.4007 - val_acc: 0.8794\n",
            "Epoch 655/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2240 - acc: 0.9161 - val_loss: 0.3904 - val_acc: 0.8873\n",
            "Epoch 656/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2258 - acc: 0.9201 - val_loss: 0.3872 - val_acc: 0.8824\n",
            "Epoch 657/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2145 - acc: 0.9279 - val_loss: 0.4090 - val_acc: 0.8739\n",
            "Epoch 658/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2167 - acc: 0.9222 - val_loss: 0.3996 - val_acc: 0.8781\n",
            "Epoch 659/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2109 - acc: 0.9288 - val_loss: 0.4028 - val_acc: 0.8812\n",
            "Epoch 660/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2325 - acc: 0.9261 - val_loss: 0.3842 - val_acc: 0.8904\n",
            "Epoch 661/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.2323 - acc: 0.9219 - val_loss: 0.4010 - val_acc: 0.8800\n",
            "Epoch 662/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2203 - acc: 0.9279 - val_loss: 0.4232 - val_acc: 0.8702\n",
            "Epoch 663/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2170 - acc: 0.9255 - val_loss: 0.4123 - val_acc: 0.8812\n",
            "Epoch 664/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.2149 - acc: 0.9258 - val_loss: 0.3835 - val_acc: 0.8855\n",
            "Epoch 665/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2255 - acc: 0.9225 - val_loss: 0.3853 - val_acc: 0.8812\n",
            "Epoch 666/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.2331 - acc: 0.9192 - val_loss: 0.3998 - val_acc: 0.8818\n",
            "Epoch 667/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2221 - acc: 0.9198 - val_loss: 0.3898 - val_acc: 0.8922\n",
            "Epoch 668/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2217 - acc: 0.9249 - val_loss: 0.3944 - val_acc: 0.8824\n",
            "Epoch 669/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.2241 - acc: 0.9219 - val_loss: 0.3824 - val_acc: 0.8898\n",
            "Epoch 670/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2296 - acc: 0.9228 - val_loss: 0.4004 - val_acc: 0.8818\n",
            "Epoch 671/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2243 - acc: 0.9201 - val_loss: 0.4069 - val_acc: 0.8781\n",
            "Epoch 672/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2200 - acc: 0.9258 - val_loss: 0.3938 - val_acc: 0.8824\n",
            "Epoch 673/1000\n",
            "3315/3315 [==============================] - 2s 461us/step - loss: 0.2109 - acc: 0.9312 - val_loss: 0.4063 - val_acc: 0.8763\n",
            "Epoch 674/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2315 - acc: 0.9216 - val_loss: 0.4114 - val_acc: 0.8849\n",
            "Epoch 675/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2211 - acc: 0.9249 - val_loss: 0.3945 - val_acc: 0.8965\n",
            "Epoch 676/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2246 - acc: 0.9225 - val_loss: 0.3831 - val_acc: 0.8879\n",
            "Epoch 677/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.2216 - acc: 0.9237 - val_loss: 0.3890 - val_acc: 0.8910\n",
            "Epoch 678/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2364 - acc: 0.9183 - val_loss: 0.3805 - val_acc: 0.8904\n",
            "Epoch 679/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2057 - acc: 0.9339 - val_loss: 0.3957 - val_acc: 0.8830\n",
            "Epoch 680/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2109 - acc: 0.9243 - val_loss: 0.3710 - val_acc: 0.8971\n",
            "Epoch 681/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.2236 - acc: 0.9240 - val_loss: 0.3839 - val_acc: 0.8885\n",
            "Epoch 682/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2134 - acc: 0.9246 - val_loss: 0.3882 - val_acc: 0.8855\n",
            "Epoch 683/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2070 - acc: 0.9333 - val_loss: 0.3853 - val_acc: 0.8861\n",
            "Epoch 684/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2284 - acc: 0.9207 - val_loss: 0.3814 - val_acc: 0.8922\n",
            "Epoch 685/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.2167 - acc: 0.9297 - val_loss: 0.3842 - val_acc: 0.8836\n",
            "Epoch 686/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.2172 - acc: 0.9297 - val_loss: 0.3884 - val_acc: 0.8855\n",
            "Epoch 687/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.2185 - acc: 0.9270 - val_loss: 0.4035 - val_acc: 0.8818\n",
            "Epoch 688/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2118 - acc: 0.9222 - val_loss: 0.3748 - val_acc: 0.8873\n",
            "Epoch 689/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.2165 - acc: 0.9228 - val_loss: 0.3802 - val_acc: 0.8971\n",
            "Epoch 690/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2212 - acc: 0.9252 - val_loss: 0.4242 - val_acc: 0.8653\n",
            "Epoch 691/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2071 - acc: 0.9312 - val_loss: 0.3829 - val_acc: 0.8873\n",
            "Epoch 692/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2176 - acc: 0.9276 - val_loss: 0.4112 - val_acc: 0.8714\n",
            "Epoch 693/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.2139 - acc: 0.9225 - val_loss: 0.3793 - val_acc: 0.8861\n",
            "Epoch 694/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.2134 - acc: 0.9231 - val_loss: 0.3791 - val_acc: 0.8898\n",
            "Epoch 695/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.2209 - acc: 0.9240 - val_loss: 0.3828 - val_acc: 0.8849\n",
            "Epoch 696/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2262 - acc: 0.9273 - val_loss: 0.3697 - val_acc: 0.8959\n",
            "Epoch 697/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2038 - acc: 0.9288 - val_loss: 0.3877 - val_acc: 0.8873\n",
            "Epoch 698/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2214 - acc: 0.9276 - val_loss: 0.3754 - val_acc: 0.8947\n",
            "Epoch 699/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.2213 - acc: 0.9195 - val_loss: 0.3762 - val_acc: 0.8904\n",
            "Epoch 700/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2023 - acc: 0.9294 - val_loss: 0.3987 - val_acc: 0.8885\n",
            "Epoch 701/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2203 - acc: 0.9246 - val_loss: 0.4000 - val_acc: 0.8726\n",
            "Epoch 702/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1990 - acc: 0.9354 - val_loss: 0.3912 - val_acc: 0.8830\n",
            "Epoch 703/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2112 - acc: 0.9303 - val_loss: 0.4040 - val_acc: 0.8708\n",
            "Epoch 704/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2137 - acc: 0.9261 - val_loss: 0.3853 - val_acc: 0.8977\n",
            "Epoch 705/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1995 - acc: 0.9288 - val_loss: 0.3890 - val_acc: 0.8910\n",
            "Epoch 706/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.2110 - acc: 0.9258 - val_loss: 0.3675 - val_acc: 0.8971\n",
            "Epoch 707/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.2027 - acc: 0.9315 - val_loss: 0.3817 - val_acc: 0.8885\n",
            "Epoch 708/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.2144 - acc: 0.9267 - val_loss: 0.3694 - val_acc: 0.8928\n",
            "Epoch 709/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2085 - acc: 0.9267 - val_loss: 0.3905 - val_acc: 0.8836\n",
            "Epoch 710/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.2017 - acc: 0.9370 - val_loss: 0.3767 - val_acc: 0.8947\n",
            "Epoch 711/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2060 - acc: 0.9306 - val_loss: 0.3818 - val_acc: 0.8812\n",
            "Epoch 712/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.2123 - acc: 0.9291 - val_loss: 0.3637 - val_acc: 0.8941\n",
            "Epoch 713/1000\n",
            "3315/3315 [==============================] - 2s 508us/step - loss: 0.2091 - acc: 0.9261 - val_loss: 0.3625 - val_acc: 0.8996\n",
            "Epoch 714/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.2099 - acc: 0.9321 - val_loss: 0.3777 - val_acc: 0.8867\n",
            "Epoch 715/1000\n",
            "3315/3315 [==============================] - 2s 518us/step - loss: 0.1971 - acc: 0.9342 - val_loss: 0.3692 - val_acc: 0.8922\n",
            "Epoch 716/1000\n",
            "3315/3315 [==============================] - 2s 513us/step - loss: 0.2018 - acc: 0.9309 - val_loss: 0.3744 - val_acc: 0.8959\n",
            "Epoch 717/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 0.2073 - acc: 0.9324 - val_loss: 0.3915 - val_acc: 0.8830\n",
            "Epoch 718/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.2081 - acc: 0.9288 - val_loss: 0.3850 - val_acc: 0.8812\n",
            "Epoch 719/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.2076 - acc: 0.9291 - val_loss: 0.3703 - val_acc: 0.8941\n",
            "Epoch 720/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2036 - acc: 0.9348 - val_loss: 0.3778 - val_acc: 0.8892\n",
            "Epoch 721/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1933 - acc: 0.9351 - val_loss: 0.3904 - val_acc: 0.8885\n",
            "Epoch 722/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1996 - acc: 0.9373 - val_loss: 0.3953 - val_acc: 0.8855\n",
            "Epoch 723/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1892 - acc: 0.9357 - val_loss: 0.3676 - val_acc: 0.8990\n",
            "Epoch 724/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2155 - acc: 0.9327 - val_loss: 0.3724 - val_acc: 0.8965\n",
            "Epoch 725/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2087 - acc: 0.9315 - val_loss: 0.3571 - val_acc: 0.8959\n",
            "Epoch 726/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.2083 - acc: 0.9255 - val_loss: 0.3769 - val_acc: 0.8867\n",
            "Epoch 727/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.2049 - acc: 0.9330 - val_loss: 0.4001 - val_acc: 0.8892\n",
            "Epoch 728/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1803 - acc: 0.9427 - val_loss: 0.4047 - val_acc: 0.8757\n",
            "Epoch 729/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2044 - acc: 0.9348 - val_loss: 0.3664 - val_acc: 0.8922\n",
            "Epoch 730/1000\n",
            "3315/3315 [==============================] - 2s 497us/step - loss: 0.2157 - acc: 0.9306 - val_loss: 0.3685 - val_acc: 0.8879\n",
            "Epoch 731/1000\n",
            "3315/3315 [==============================] - 2s 495us/step - loss: 0.2001 - acc: 0.9321 - val_loss: 0.3978 - val_acc: 0.8898\n",
            "Epoch 732/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.2058 - acc: 0.9240 - val_loss: 0.3804 - val_acc: 0.8892\n",
            "Epoch 733/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.2081 - acc: 0.9297 - val_loss: 0.3735 - val_acc: 0.8855\n",
            "Epoch 734/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.1955 - acc: 0.9330 - val_loss: 0.3613 - val_acc: 0.8953\n",
            "Epoch 735/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.1851 - acc: 0.9373 - val_loss: 0.3946 - val_acc: 0.8892\n",
            "Epoch 736/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1937 - acc: 0.9327 - val_loss: 0.3536 - val_acc: 0.9051\n",
            "Epoch 737/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1948 - acc: 0.9315 - val_loss: 0.3679 - val_acc: 0.8996\n",
            "Epoch 738/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.2065 - acc: 0.9285 - val_loss: 0.3755 - val_acc: 0.8990\n",
            "Epoch 739/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.2020 - acc: 0.9288 - val_loss: 0.3671 - val_acc: 0.8947\n",
            "Epoch 740/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1851 - acc: 0.9360 - val_loss: 0.3676 - val_acc: 0.8947\n",
            "Epoch 741/1000\n",
            "3315/3315 [==============================] - 2s 504us/step - loss: 0.2025 - acc: 0.9273 - val_loss: 0.3726 - val_acc: 0.8959\n",
            "Epoch 742/1000\n",
            "3315/3315 [==============================] - 2s 530us/step - loss: 0.1985 - acc: 0.9300 - val_loss: 0.3545 - val_acc: 0.8965\n",
            "Epoch 743/1000\n",
            "3315/3315 [==============================] - 2s 536us/step - loss: 0.1990 - acc: 0.9360 - val_loss: 0.3769 - val_acc: 0.8885\n",
            "Epoch 744/1000\n",
            "3315/3315 [==============================] - 2s 511us/step - loss: 0.1984 - acc: 0.9321 - val_loss: 0.3739 - val_acc: 0.8885\n",
            "Epoch 745/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.1929 - acc: 0.9360 - val_loss: 0.3814 - val_acc: 0.8922\n",
            "Epoch 746/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.2044 - acc: 0.9336 - val_loss: 0.3684 - val_acc: 0.8904\n",
            "Epoch 747/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1969 - acc: 0.9367 - val_loss: 0.3755 - val_acc: 0.8861\n",
            "Epoch 748/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.2052 - acc: 0.9300 - val_loss: 0.3648 - val_acc: 0.8934\n",
            "Epoch 749/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.1914 - acc: 0.9342 - val_loss: 0.3651 - val_acc: 0.9045\n",
            "Epoch 750/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1975 - acc: 0.9324 - val_loss: 0.3673 - val_acc: 0.8873\n",
            "Epoch 751/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1903 - acc: 0.9345 - val_loss: 0.3577 - val_acc: 0.9002\n",
            "Epoch 752/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.2013 - acc: 0.9294 - val_loss: 0.4113 - val_acc: 0.8739\n",
            "Epoch 753/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1965 - acc: 0.9273 - val_loss: 0.3744 - val_acc: 0.8885\n",
            "Epoch 754/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1986 - acc: 0.9300 - val_loss: 0.3979 - val_acc: 0.8885\n",
            "Epoch 755/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1917 - acc: 0.9354 - val_loss: 0.3646 - val_acc: 0.9051\n",
            "Epoch 756/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1834 - acc: 0.9391 - val_loss: 0.3603 - val_acc: 0.9020\n",
            "Epoch 757/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1853 - acc: 0.9373 - val_loss: 0.3720 - val_acc: 0.8947\n",
            "Epoch 758/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1910 - acc: 0.9379 - val_loss: 0.3899 - val_acc: 0.8916\n",
            "Epoch 759/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1946 - acc: 0.9351 - val_loss: 0.3842 - val_acc: 0.8885\n",
            "Epoch 760/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1892 - acc: 0.9354 - val_loss: 0.3761 - val_acc: 0.9032\n",
            "Epoch 761/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1841 - acc: 0.9354 - val_loss: 0.3696 - val_acc: 0.8965\n",
            "Epoch 762/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1896 - acc: 0.9391 - val_loss: 0.3646 - val_acc: 0.8953\n",
            "Epoch 763/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1845 - acc: 0.9363 - val_loss: 0.3470 - val_acc: 0.8990\n",
            "Epoch 764/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1843 - acc: 0.9388 - val_loss: 0.3772 - val_acc: 0.8898\n",
            "Epoch 765/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1875 - acc: 0.9403 - val_loss: 0.3706 - val_acc: 0.8922\n",
            "Epoch 766/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1676 - acc: 0.9424 - val_loss: 0.3498 - val_acc: 0.8996\n",
            "Epoch 767/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1898 - acc: 0.9367 - val_loss: 0.3533 - val_acc: 0.9008\n",
            "Epoch 768/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1874 - acc: 0.9315 - val_loss: 0.3487 - val_acc: 0.9026\n",
            "Epoch 769/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1865 - acc: 0.9382 - val_loss: 0.3469 - val_acc: 0.8996\n",
            "Epoch 770/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1859 - acc: 0.9367 - val_loss: 0.3617 - val_acc: 0.9051\n",
            "Epoch 771/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1928 - acc: 0.9357 - val_loss: 0.3878 - val_acc: 0.8885\n",
            "Epoch 772/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1889 - acc: 0.9397 - val_loss: 0.3618 - val_acc: 0.8990\n",
            "Epoch 773/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1955 - acc: 0.9309 - val_loss: 0.3431 - val_acc: 0.9045\n",
            "Epoch 774/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1823 - acc: 0.9379 - val_loss: 0.3622 - val_acc: 0.9020\n",
            "Epoch 775/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1741 - acc: 0.9406 - val_loss: 0.3544 - val_acc: 0.8983\n",
            "Epoch 776/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.1880 - acc: 0.9436 - val_loss: 0.3461 - val_acc: 0.9039\n",
            "Epoch 777/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.1952 - acc: 0.9318 - val_loss: 0.3730 - val_acc: 0.8959\n",
            "Epoch 778/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1879 - acc: 0.9406 - val_loss: 0.3558 - val_acc: 0.8867\n",
            "Epoch 779/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1941 - acc: 0.9321 - val_loss: 0.3600 - val_acc: 0.9008\n",
            "Epoch 780/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1903 - acc: 0.9351 - val_loss: 0.3793 - val_acc: 0.8916\n",
            "Epoch 781/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.1796 - acc: 0.9418 - val_loss: 0.3813 - val_acc: 0.8928\n",
            "Epoch 782/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1934 - acc: 0.9324 - val_loss: 0.3590 - val_acc: 0.8904\n",
            "Epoch 783/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1978 - acc: 0.9357 - val_loss: 0.3528 - val_acc: 0.9051\n",
            "Epoch 784/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1852 - acc: 0.9367 - val_loss: 0.3757 - val_acc: 0.8941\n",
            "Epoch 785/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1935 - acc: 0.9360 - val_loss: 0.3629 - val_acc: 0.8971\n",
            "Epoch 786/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1661 - acc: 0.9427 - val_loss: 0.3704 - val_acc: 0.9002\n",
            "Epoch 787/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1866 - acc: 0.9388 - val_loss: 0.3581 - val_acc: 0.8977\n",
            "Epoch 788/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.1743 - acc: 0.9430 - val_loss: 0.3809 - val_acc: 0.8855\n",
            "Epoch 789/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1816 - acc: 0.9373 - val_loss: 0.3882 - val_acc: 0.8885\n",
            "Epoch 790/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1615 - acc: 0.9439 - val_loss: 0.3747 - val_acc: 0.8990\n",
            "Epoch 791/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.1813 - acc: 0.9385 - val_loss: 0.3966 - val_acc: 0.8928\n",
            "Epoch 792/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.1796 - acc: 0.9397 - val_loss: 0.3764 - val_acc: 0.8996\n",
            "Epoch 793/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.1915 - acc: 0.9351 - val_loss: 0.3597 - val_acc: 0.9081\n",
            "Epoch 794/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1753 - acc: 0.9424 - val_loss: 0.3655 - val_acc: 0.8996\n",
            "Epoch 795/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1891 - acc: 0.9357 - val_loss: 0.3711 - val_acc: 0.9002\n",
            "Epoch 796/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1753 - acc: 0.9415 - val_loss: 0.3482 - val_acc: 0.9063\n",
            "Epoch 797/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1724 - acc: 0.9433 - val_loss: 0.3839 - val_acc: 0.8959\n",
            "Epoch 798/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1891 - acc: 0.9336 - val_loss: 0.3563 - val_acc: 0.8990\n",
            "Epoch 799/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1899 - acc: 0.9351 - val_loss: 0.3599 - val_acc: 0.9045\n",
            "Epoch 800/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1777 - acc: 0.9406 - val_loss: 0.3715 - val_acc: 0.9014\n",
            "Epoch 801/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1756 - acc: 0.9424 - val_loss: 0.3591 - val_acc: 0.8977\n",
            "Epoch 802/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1744 - acc: 0.9415 - val_loss: 0.3508 - val_acc: 0.9118\n",
            "Epoch 803/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1787 - acc: 0.9406 - val_loss: 0.3825 - val_acc: 0.8922\n",
            "Epoch 804/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1826 - acc: 0.9394 - val_loss: 0.3540 - val_acc: 0.8996\n",
            "Epoch 805/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1878 - acc: 0.9370 - val_loss: 0.3510 - val_acc: 0.9075\n",
            "Epoch 806/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1720 - acc: 0.9412 - val_loss: 0.3711 - val_acc: 0.8947\n",
            "Epoch 807/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.1654 - acc: 0.9439 - val_loss: 0.3556 - val_acc: 0.9020\n",
            "Epoch 808/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1704 - acc: 0.9451 - val_loss: 0.3488 - val_acc: 0.8990\n",
            "Epoch 809/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1672 - acc: 0.9451 - val_loss: 0.3472 - val_acc: 0.9069\n",
            "Epoch 810/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.1776 - acc: 0.9418 - val_loss: 0.3531 - val_acc: 0.9081\n",
            "Epoch 811/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1798 - acc: 0.9394 - val_loss: 0.3733 - val_acc: 0.8922\n",
            "Epoch 812/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.1767 - acc: 0.9400 - val_loss: 0.3617 - val_acc: 0.9039\n",
            "Epoch 813/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1764 - acc: 0.9457 - val_loss: 0.3576 - val_acc: 0.8971\n",
            "Epoch 814/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1587 - acc: 0.9454 - val_loss: 0.3521 - val_acc: 0.9063\n",
            "Epoch 815/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1678 - acc: 0.9430 - val_loss: 0.3641 - val_acc: 0.8941\n",
            "Epoch 816/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1783 - acc: 0.9418 - val_loss: 0.3587 - val_acc: 0.8996\n",
            "Epoch 817/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1766 - acc: 0.9382 - val_loss: 0.3556 - val_acc: 0.9057\n",
            "Epoch 818/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1623 - acc: 0.9451 - val_loss: 0.3753 - val_acc: 0.9002\n",
            "Epoch 819/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1665 - acc: 0.9460 - val_loss: 0.3588 - val_acc: 0.8983\n",
            "Epoch 820/1000\n",
            "3315/3315 [==============================] - 2s 462us/step - loss: 0.1740 - acc: 0.9400 - val_loss: 0.3393 - val_acc: 0.9045\n",
            "Epoch 821/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1609 - acc: 0.9460 - val_loss: 0.3459 - val_acc: 0.9094\n",
            "Epoch 822/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1800 - acc: 0.9397 - val_loss: 0.3683 - val_acc: 0.8996\n",
            "Epoch 823/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1878 - acc: 0.9336 - val_loss: 0.3680 - val_acc: 0.9002\n",
            "Epoch 824/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1751 - acc: 0.9412 - val_loss: 0.3662 - val_acc: 0.8953\n",
            "Epoch 825/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1773 - acc: 0.9382 - val_loss: 0.3419 - val_acc: 0.9045\n",
            "Epoch 826/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.1801 - acc: 0.9388 - val_loss: 0.3607 - val_acc: 0.9032\n",
            "Epoch 827/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1815 - acc: 0.9418 - val_loss: 0.3767 - val_acc: 0.8947\n",
            "Epoch 828/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1709 - acc: 0.9415 - val_loss: 0.3565 - val_acc: 0.9002\n",
            "Epoch 829/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1644 - acc: 0.9415 - val_loss: 0.3572 - val_acc: 0.8990\n",
            "Epoch 830/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.1624 - acc: 0.9418 - val_loss: 0.3748 - val_acc: 0.8934\n",
            "Epoch 831/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1412 - acc: 0.9548 - val_loss: 0.3478 - val_acc: 0.9045\n",
            "Epoch 832/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.1711 - acc: 0.9397 - val_loss: 0.3813 - val_acc: 0.8965\n",
            "Epoch 833/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1621 - acc: 0.9463 - val_loss: 0.3631 - val_acc: 0.9008\n",
            "Epoch 834/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1654 - acc: 0.9433 - val_loss: 0.3508 - val_acc: 0.9026\n",
            "Epoch 835/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1653 - acc: 0.9379 - val_loss: 0.3349 - val_acc: 0.9100\n",
            "Epoch 836/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1617 - acc: 0.9421 - val_loss: 0.3571 - val_acc: 0.9100\n",
            "Epoch 837/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1748 - acc: 0.9430 - val_loss: 0.3594 - val_acc: 0.9014\n",
            "Epoch 838/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1752 - acc: 0.9406 - val_loss: 0.3442 - val_acc: 0.9075\n",
            "Epoch 839/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1621 - acc: 0.9496 - val_loss: 0.3510 - val_acc: 0.9051\n",
            "Epoch 840/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.1504 - acc: 0.9502 - val_loss: 0.3432 - val_acc: 0.9032\n",
            "Epoch 841/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1768 - acc: 0.9436 - val_loss: 0.3539 - val_acc: 0.9032\n",
            "Epoch 842/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1696 - acc: 0.9442 - val_loss: 0.3757 - val_acc: 0.8953\n",
            "Epoch 843/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1654 - acc: 0.9472 - val_loss: 0.3538 - val_acc: 0.9014\n",
            "Epoch 844/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1552 - acc: 0.9472 - val_loss: 0.3610 - val_acc: 0.9026\n",
            "Epoch 845/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.1698 - acc: 0.9418 - val_loss: 0.3569 - val_acc: 0.9045\n",
            "Epoch 846/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.1626 - acc: 0.9442 - val_loss: 0.3500 - val_acc: 0.9045\n",
            "Epoch 847/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1589 - acc: 0.9478 - val_loss: 0.3428 - val_acc: 0.9051\n",
            "Epoch 848/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1700 - acc: 0.9427 - val_loss: 0.3591 - val_acc: 0.8990\n",
            "Epoch 849/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1669 - acc: 0.9436 - val_loss: 0.3718 - val_acc: 0.8990\n",
            "Epoch 850/1000\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.1570 - acc: 0.9436 - val_loss: 0.3371 - val_acc: 0.9002\n",
            "Epoch 851/1000\n",
            "3315/3315 [==============================] - 2s 455us/step - loss: 0.1837 - acc: 0.9373 - val_loss: 0.3505 - val_acc: 0.9088\n",
            "Epoch 852/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1512 - acc: 0.9505 - val_loss: 0.3421 - val_acc: 0.9088\n",
            "Epoch 853/1000\n",
            "3315/3315 [==============================] - 2s 459us/step - loss: 0.1629 - acc: 0.9439 - val_loss: 0.3318 - val_acc: 0.9112\n",
            "Epoch 854/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.1607 - acc: 0.9478 - val_loss: 0.3538 - val_acc: 0.9051\n",
            "Epoch 855/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1647 - acc: 0.9436 - val_loss: 0.3488 - val_acc: 0.9014\n",
            "Epoch 856/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1638 - acc: 0.9430 - val_loss: 0.3616 - val_acc: 0.8996\n",
            "Epoch 857/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1749 - acc: 0.9427 - val_loss: 0.3522 - val_acc: 0.9075\n",
            "Epoch 858/1000\n",
            "3315/3315 [==============================] - 2s 460us/step - loss: 0.1606 - acc: 0.9472 - val_loss: 0.3571 - val_acc: 0.9051\n",
            "Epoch 859/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1664 - acc: 0.9439 - val_loss: 0.3725 - val_acc: 0.9032\n",
            "Epoch 860/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1695 - acc: 0.9418 - val_loss: 0.3610 - val_acc: 0.8977\n",
            "Epoch 861/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1716 - acc: 0.9421 - val_loss: 0.3372 - val_acc: 0.9088\n",
            "Epoch 862/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1641 - acc: 0.9433 - val_loss: 0.3737 - val_acc: 0.9051\n",
            "Epoch 863/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1568 - acc: 0.9484 - val_loss: 0.3352 - val_acc: 0.9143\n",
            "Epoch 864/1000\n",
            "3315/3315 [==============================] - 2s 488us/step - loss: 0.1648 - acc: 0.9442 - val_loss: 0.3400 - val_acc: 0.9106\n",
            "Epoch 865/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1596 - acc: 0.9433 - val_loss: 0.3417 - val_acc: 0.9124\n",
            "Epoch 866/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1562 - acc: 0.9475 - val_loss: 0.3522 - val_acc: 0.9026\n",
            "Epoch 867/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1530 - acc: 0.9460 - val_loss: 0.3656 - val_acc: 0.9039\n",
            "Epoch 868/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1601 - acc: 0.9460 - val_loss: 0.3558 - val_acc: 0.9118\n",
            "Epoch 869/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1713 - acc: 0.9388 - val_loss: 0.3268 - val_acc: 0.9130\n",
            "Epoch 870/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1748 - acc: 0.9409 - val_loss: 0.3671 - val_acc: 0.9039\n",
            "Epoch 871/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1634 - acc: 0.9445 - val_loss: 0.3498 - val_acc: 0.9075\n",
            "Epoch 872/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1568 - acc: 0.9502 - val_loss: 0.3409 - val_acc: 0.9081\n",
            "Epoch 873/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1855 - acc: 0.9367 - val_loss: 0.3579 - val_acc: 0.9026\n",
            "Epoch 874/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1720 - acc: 0.9394 - val_loss: 0.3472 - val_acc: 0.9014\n",
            "Epoch 875/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1526 - acc: 0.9514 - val_loss: 0.3525 - val_acc: 0.9002\n",
            "Epoch 876/1000\n",
            "3315/3315 [==============================] - 2s 465us/step - loss: 0.1701 - acc: 0.9385 - val_loss: 0.3548 - val_acc: 0.9026\n",
            "Epoch 877/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1537 - acc: 0.9430 - val_loss: 0.3501 - val_acc: 0.9051\n",
            "Epoch 878/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1512 - acc: 0.9451 - val_loss: 0.3417 - val_acc: 0.9032\n",
            "Epoch 879/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1616 - acc: 0.9421 - val_loss: 0.3509 - val_acc: 0.9008\n",
            "Epoch 880/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1502 - acc: 0.9496 - val_loss: 0.3169 - val_acc: 0.9161\n",
            "Epoch 881/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.1586 - acc: 0.9457 - val_loss: 0.3450 - val_acc: 0.9063\n",
            "Epoch 882/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1595 - acc: 0.9418 - val_loss: 0.3490 - val_acc: 0.9026\n",
            "Epoch 883/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1588 - acc: 0.9445 - val_loss: 0.3537 - val_acc: 0.9057\n",
            "Epoch 884/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1529 - acc: 0.9466 - val_loss: 0.3394 - val_acc: 0.9124\n",
            "Epoch 885/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1572 - acc: 0.9448 - val_loss: 0.3499 - val_acc: 0.9081\n",
            "Epoch 886/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1532 - acc: 0.9499 - val_loss: 0.3334 - val_acc: 0.9106\n",
            "Epoch 887/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1627 - acc: 0.9415 - val_loss: 0.3343 - val_acc: 0.9118\n",
            "Epoch 888/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1483 - acc: 0.9487 - val_loss: 0.3471 - val_acc: 0.9137\n",
            "Epoch 889/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1568 - acc: 0.9451 - val_loss: 0.3279 - val_acc: 0.9118\n",
            "Epoch 890/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1344 - acc: 0.9526 - val_loss: 0.3414 - val_acc: 0.9088\n",
            "Epoch 891/1000\n",
            "3315/3315 [==============================] - 2s 456us/step - loss: 0.1536 - acc: 0.9499 - val_loss: 0.3589 - val_acc: 0.9063\n",
            "Epoch 892/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.1592 - acc: 0.9436 - val_loss: 0.3303 - val_acc: 0.9081\n",
            "Epoch 893/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1561 - acc: 0.9487 - val_loss: 0.3759 - val_acc: 0.8996\n",
            "Epoch 894/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1505 - acc: 0.9466 - val_loss: 0.3426 - val_acc: 0.9100\n",
            "Epoch 895/1000\n",
            "3315/3315 [==============================] - 2s 463us/step - loss: 0.1525 - acc: 0.9490 - val_loss: 0.3733 - val_acc: 0.9045\n",
            "Epoch 896/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1529 - acc: 0.9460 - val_loss: 0.3394 - val_acc: 0.9118\n",
            "Epoch 897/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1588 - acc: 0.9463 - val_loss: 0.3515 - val_acc: 0.9063\n",
            "Epoch 898/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1509 - acc: 0.9478 - val_loss: 0.3279 - val_acc: 0.9149\n",
            "Epoch 899/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1359 - acc: 0.9584 - val_loss: 0.3727 - val_acc: 0.8959\n",
            "Epoch 900/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.1717 - acc: 0.9433 - val_loss: 0.3347 - val_acc: 0.9130\n",
            "Epoch 901/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1657 - acc: 0.9454 - val_loss: 0.3359 - val_acc: 0.9167\n",
            "Epoch 902/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1499 - acc: 0.9472 - val_loss: 0.3348 - val_acc: 0.9130\n",
            "Epoch 903/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1631 - acc: 0.9490 - val_loss: 0.3302 - val_acc: 0.9149\n",
            "Epoch 904/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1414 - acc: 0.9535 - val_loss: 0.3336 - val_acc: 0.9118\n",
            "Epoch 905/1000\n",
            "3315/3315 [==============================] - 2s 481us/step - loss: 0.1574 - acc: 0.9466 - val_loss: 0.3387 - val_acc: 0.9081\n",
            "Epoch 906/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1568 - acc: 0.9427 - val_loss: 0.3456 - val_acc: 0.9057\n",
            "Epoch 907/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1519 - acc: 0.9478 - val_loss: 0.3640 - val_acc: 0.8971\n",
            "Epoch 908/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1526 - acc: 0.9487 - val_loss: 0.3438 - val_acc: 0.9063\n",
            "Epoch 909/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1499 - acc: 0.9490 - val_loss: 0.3577 - val_acc: 0.9014\n",
            "Epoch 910/1000\n",
            "3315/3315 [==============================] - 2s 507us/step - loss: 0.1486 - acc: 0.9517 - val_loss: 0.3433 - val_acc: 0.9094\n",
            "Epoch 911/1000\n",
            "3315/3315 [==============================] - 2s 512us/step - loss: 0.1453 - acc: 0.9508 - val_loss: 0.3545 - val_acc: 0.8977\n",
            "Epoch 912/1000\n",
            "3315/3315 [==============================] - 2s 518us/step - loss: 0.1470 - acc: 0.9490 - val_loss: 0.3536 - val_acc: 0.9020\n",
            "Epoch 913/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.1548 - acc: 0.9472 - val_loss: 0.3497 - val_acc: 0.9118\n",
            "Epoch 914/1000\n",
            "3315/3315 [==============================] - 2s 515us/step - loss: 0.1548 - acc: 0.9448 - val_loss: 0.3398 - val_acc: 0.9112\n",
            "Epoch 915/1000\n",
            "3315/3315 [==============================] - 2s 509us/step - loss: 0.1482 - acc: 0.9487 - val_loss: 0.3372 - val_acc: 0.9100\n",
            "Epoch 916/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1575 - acc: 0.9472 - val_loss: 0.3454 - val_acc: 0.9063\n",
            "Epoch 917/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.1587 - acc: 0.9487 - val_loss: 0.3273 - val_acc: 0.9118\n",
            "Epoch 918/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1576 - acc: 0.9472 - val_loss: 0.3606 - val_acc: 0.9081\n",
            "Epoch 919/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1399 - acc: 0.9548 - val_loss: 0.3382 - val_acc: 0.9179\n",
            "Epoch 920/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1462 - acc: 0.9508 - val_loss: 0.3398 - val_acc: 0.9137\n",
            "Epoch 921/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1476 - acc: 0.9505 - val_loss: 0.3403 - val_acc: 0.9100\n",
            "Epoch 922/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1530 - acc: 0.9475 - val_loss: 0.3423 - val_acc: 0.9002\n",
            "Epoch 923/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.1526 - acc: 0.9463 - val_loss: 0.3424 - val_acc: 0.9051\n",
            "Epoch 924/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1467 - acc: 0.9505 - val_loss: 0.3270 - val_acc: 0.9155\n",
            "Epoch 925/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1410 - acc: 0.9496 - val_loss: 0.3310 - val_acc: 0.9167\n",
            "Epoch 926/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.1382 - acc: 0.9520 - val_loss: 0.3418 - val_acc: 0.9032\n",
            "Epoch 927/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.1471 - acc: 0.9466 - val_loss: 0.3623 - val_acc: 0.9020\n",
            "Epoch 928/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1470 - acc: 0.9499 - val_loss: 0.3603 - val_acc: 0.9045\n",
            "Epoch 929/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1443 - acc: 0.9517 - val_loss: 0.3546 - val_acc: 0.9094\n",
            "Epoch 930/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1410 - acc: 0.9544 - val_loss: 0.3567 - val_acc: 0.8983\n",
            "Epoch 931/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1482 - acc: 0.9511 - val_loss: 0.3375 - val_acc: 0.9167\n",
            "Epoch 932/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1575 - acc: 0.9427 - val_loss: 0.3207 - val_acc: 0.9198\n",
            "Epoch 933/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1592 - acc: 0.9502 - val_loss: 0.3283 - val_acc: 0.9124\n",
            "Epoch 934/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.1521 - acc: 0.9502 - val_loss: 0.3577 - val_acc: 0.9045\n",
            "Epoch 935/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1430 - acc: 0.9505 - val_loss: 0.3345 - val_acc: 0.9112\n",
            "Epoch 936/1000\n",
            "3315/3315 [==============================] - 2s 525us/step - loss: 0.1445 - acc: 0.9493 - val_loss: 0.3495 - val_acc: 0.9057\n",
            "Epoch 937/1000\n",
            "3315/3315 [==============================] - 2s 551us/step - loss: 0.1532 - acc: 0.9496 - val_loss: 0.3514 - val_acc: 0.9045\n",
            "Epoch 938/1000\n",
            "3315/3315 [==============================] - 2s 526us/step - loss: 0.1550 - acc: 0.9457 - val_loss: 0.3325 - val_acc: 0.9161\n",
            "Epoch 939/1000\n",
            "3315/3315 [==============================] - 2s 498us/step - loss: 0.1330 - acc: 0.9541 - val_loss: 0.3620 - val_acc: 0.8910\n",
            "Epoch 940/1000\n",
            "3315/3315 [==============================] - 2s 487us/step - loss: 0.1469 - acc: 0.9481 - val_loss: 0.3540 - val_acc: 0.9130\n",
            "Epoch 941/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.1365 - acc: 0.9523 - val_loss: 0.3505 - val_acc: 0.9051\n",
            "Epoch 942/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1469 - acc: 0.9499 - val_loss: 0.3430 - val_acc: 0.9088\n",
            "Epoch 943/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1437 - acc: 0.9520 - val_loss: 0.3339 - val_acc: 0.9112\n",
            "Epoch 944/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1438 - acc: 0.9572 - val_loss: 0.3618 - val_acc: 0.9088\n",
            "Epoch 945/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1351 - acc: 0.9569 - val_loss: 0.3411 - val_acc: 0.9179\n",
            "Epoch 946/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1499 - acc: 0.9478 - val_loss: 0.3430 - val_acc: 0.9137\n",
            "Epoch 947/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1530 - acc: 0.9481 - val_loss: 0.3673 - val_acc: 0.9051\n",
            "Epoch 948/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1459 - acc: 0.9520 - val_loss: 0.3507 - val_acc: 0.9094\n",
            "Epoch 949/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.1483 - acc: 0.9466 - val_loss: 0.3405 - val_acc: 0.9069\n",
            "Epoch 950/1000\n",
            "3315/3315 [==============================] - 2s 468us/step - loss: 0.1387 - acc: 0.9523 - val_loss: 0.3413 - val_acc: 0.8996\n",
            "Epoch 951/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1452 - acc: 0.9469 - val_loss: 0.3613 - val_acc: 0.9069\n",
            "Epoch 952/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1466 - acc: 0.9499 - val_loss: 0.3273 - val_acc: 0.9143\n",
            "Epoch 953/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1429 - acc: 0.9469 - val_loss: 0.3327 - val_acc: 0.9137\n",
            "Epoch 954/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1408 - acc: 0.9505 - val_loss: 0.3286 - val_acc: 0.9161\n",
            "Epoch 955/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1534 - acc: 0.9505 - val_loss: 0.3458 - val_acc: 0.9075\n",
            "Epoch 956/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1335 - acc: 0.9520 - val_loss: 0.3406 - val_acc: 0.9118\n",
            "Epoch 957/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1416 - acc: 0.9557 - val_loss: 0.3520 - val_acc: 0.9112\n",
            "Epoch 958/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1405 - acc: 0.9535 - val_loss: 0.3312 - val_acc: 0.9088\n",
            "Epoch 959/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1477 - acc: 0.9460 - val_loss: 0.3344 - val_acc: 0.9088\n",
            "Epoch 960/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1482 - acc: 0.9481 - val_loss: 0.3557 - val_acc: 0.9039\n",
            "Epoch 961/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.1482 - acc: 0.9493 - val_loss: 0.3526 - val_acc: 0.9057\n",
            "Epoch 962/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1440 - acc: 0.9535 - val_loss: 0.3499 - val_acc: 0.9032\n",
            "Epoch 963/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1420 - acc: 0.9557 - val_loss: 0.3559 - val_acc: 0.9063\n",
            "Epoch 964/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1204 - acc: 0.9572 - val_loss: 0.3798 - val_acc: 0.9020\n",
            "Epoch 965/1000\n",
            "3315/3315 [==============================] - 2s 471us/step - loss: 0.1367 - acc: 0.9560 - val_loss: 0.3283 - val_acc: 0.9124\n",
            "Epoch 966/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1353 - acc: 0.9511 - val_loss: 0.3652 - val_acc: 0.9081\n",
            "Epoch 967/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1405 - acc: 0.9517 - val_loss: 0.3539 - val_acc: 0.9051\n",
            "Epoch 968/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1319 - acc: 0.9529 - val_loss: 0.3825 - val_acc: 0.8959\n",
            "Epoch 969/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1379 - acc: 0.9538 - val_loss: 0.3433 - val_acc: 0.9088\n",
            "Epoch 970/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1502 - acc: 0.9478 - val_loss: 0.3348 - val_acc: 0.9063\n",
            "Epoch 971/1000\n",
            "3315/3315 [==============================] - 2s 475us/step - loss: 0.1322 - acc: 0.9563 - val_loss: 0.3418 - val_acc: 0.9106\n",
            "Epoch 972/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1433 - acc: 0.9538 - val_loss: 0.3280 - val_acc: 0.9118\n",
            "Epoch 973/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1459 - acc: 0.9502 - val_loss: 0.3263 - val_acc: 0.9094\n",
            "Epoch 974/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1369 - acc: 0.9548 - val_loss: 0.3183 - val_acc: 0.9210\n",
            "Epoch 975/1000\n",
            "3315/3315 [==============================] - 2s 469us/step - loss: 0.1368 - acc: 0.9526 - val_loss: 0.3580 - val_acc: 0.9075\n",
            "Epoch 976/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.1442 - acc: 0.9496 - val_loss: 0.3571 - val_acc: 0.9149\n",
            "Epoch 977/1000\n",
            "3315/3315 [==============================] - 2s 472us/step - loss: 0.1347 - acc: 0.9544 - val_loss: 0.3392 - val_acc: 0.9118\n",
            "Epoch 978/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1380 - acc: 0.9529 - val_loss: 0.3460 - val_acc: 0.9106\n",
            "Epoch 979/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1334 - acc: 0.9575 - val_loss: 0.3440 - val_acc: 0.9130\n",
            "Epoch 980/1000\n",
            "3315/3315 [==============================] - 2s 464us/step - loss: 0.1425 - acc: 0.9514 - val_loss: 0.3357 - val_acc: 0.9069\n",
            "Epoch 981/1000\n",
            "3315/3315 [==============================] - 2s 482us/step - loss: 0.1441 - acc: 0.9517 - val_loss: 0.3320 - val_acc: 0.9100\n",
            "Epoch 982/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.1143 - acc: 0.9569 - val_loss: 0.3286 - val_acc: 0.9063\n",
            "Epoch 983/1000\n",
            "3315/3315 [==============================] - 2s 484us/step - loss: 0.1402 - acc: 0.9517 - val_loss: 0.3232 - val_acc: 0.9094\n",
            "Epoch 984/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.1336 - acc: 0.9520 - val_loss: 0.3456 - val_acc: 0.9014\n",
            "Epoch 985/1000\n",
            "3315/3315 [==============================] - 2s 474us/step - loss: 0.1263 - acc: 0.9593 - val_loss: 0.3244 - val_acc: 0.9167\n",
            "Epoch 986/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1377 - acc: 0.9520 - val_loss: 0.3447 - val_acc: 0.9075\n",
            "Epoch 987/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1517 - acc: 0.9532 - val_loss: 0.3618 - val_acc: 0.9045\n",
            "Epoch 988/1000\n",
            "3315/3315 [==============================] - 2s 478us/step - loss: 0.1358 - acc: 0.9532 - val_loss: 0.3326 - val_acc: 0.9167\n",
            "Epoch 989/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.1440 - acc: 0.9472 - val_loss: 0.3250 - val_acc: 0.9155\n",
            "Epoch 990/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1412 - acc: 0.9557 - val_loss: 0.3197 - val_acc: 0.9253\n",
            "Epoch 991/1000\n",
            "3315/3315 [==============================] - 2s 467us/step - loss: 0.1208 - acc: 0.9581 - val_loss: 0.3520 - val_acc: 0.9088\n",
            "Epoch 992/1000\n",
            "3315/3315 [==============================] - 2s 466us/step - loss: 0.1445 - acc: 0.9502 - val_loss: 0.3544 - val_acc: 0.9063\n",
            "Epoch 993/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1250 - acc: 0.9596 - val_loss: 0.3496 - val_acc: 0.9149\n",
            "Epoch 994/1000\n",
            "3315/3315 [==============================] - 2s 473us/step - loss: 0.1231 - acc: 0.9620 - val_loss: 0.3677 - val_acc: 0.9045\n",
            "Epoch 995/1000\n",
            "3315/3315 [==============================] - 2s 476us/step - loss: 0.1346 - acc: 0.9529 - val_loss: 0.3444 - val_acc: 0.9198\n",
            "Epoch 996/1000\n",
            "3315/3315 [==============================] - 2s 479us/step - loss: 0.1403 - acc: 0.9532 - val_loss: 0.3303 - val_acc: 0.9124\n",
            "Epoch 997/1000\n",
            "3315/3315 [==============================] - 2s 470us/step - loss: 0.1404 - acc: 0.9502 - val_loss: 0.3318 - val_acc: 0.9192\n",
            "Epoch 998/1000\n",
            "3315/3315 [==============================] - 2s 477us/step - loss: 0.1364 - acc: 0.9496 - val_loss: 0.3369 - val_acc: 0.9130\n",
            "Epoch 999/1000\n",
            "3315/3315 [==============================] - 2s 480us/step - loss: 0.1301 - acc: 0.9593 - val_loss: 0.3350 - val_acc: 0.9137\n",
            "Epoch 1000/1000\n",
            "3315/3315 [==============================] - 2s 483us/step - loss: 0.1260 - acc: 0.9557 - val_loss: 0.3365 - val_acc: 0.9124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mFytY6LDzgJ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the loss:"
      ]
    },
    {
      "metadata": {
        "id": "TFz4ClZov9gZ",
        "colab_type": "code",
        "outputId": "8d9a42fb-74d2-48b2-82e5-d92db0b9fd79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEVCAYAAAAM3jVmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUeiLiFQRhb6ISBVR6IuIVBGFvohIFfn/svgHtWLfoocAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Vf1W7LgP2DA5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "And now let's plot the accuracy:"
      ]
    },
    {
      "metadata": {
        "id": "8yyFBt7ASPUe",
        "colab_type": "code",
        "outputId": "c086fac3-5fc9-4c72-e38e-70db8ebdb004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(cnnhistory.history['acc'])\n",
        "plt.plot(cnnhistory.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd4W9X5wPGvJEvynvGM48RZJ8PZ\neydAmKEQoKwCZbeFsmdbaIFCKWVTZlsK5dcCZUPZKwmEBLITsk6WEyeOYzuO97ak3x9Xlq14xE4s\ny5bez/PwoHvuOsd2zqt77hkml8uFEEKI4GP2dwaEEEL4hwQAIYQIUhIAhBAiSEkAEEKIICUBQAgh\ngpQEACGECFISAETQUUr9Qyl1zxGOuVQp9WU3ZUkIv5AAIIQQQSrE3xkQoj1KqQHAcuBx4ArABFwC\n3A2MBT7TWl/uPvanwB8w/q73A1dprXcqpRKA14AhwGagCtjnPmcE8ByQCtQCl2mtVx0hT3cDF7nv\nswW4SGtdopQKA14AZgE1wANa63+3k/4ysENrfb/7up5tpdRu4J/Az4D5QBjwIpAAWIG7tdavuc87\nGXjUnb7N/fN5AfhBa/2I+5gsYBGQqrVu6NhPXwQ6eQIQvUEf4IDWWgEbgP8CPwdGAxcqpQYppTKA\nvwNnaq2HAR9hVIIAdwCFWutM4FrgJACllBl4D3hFaz0U+CXwvlKqzS9GSqkJwK+BSRgBxe7eBrgF\nsLnvMx94WimV1k76kaRrrZXWOgd4BPhQaz0cuBx4USllVUpFAP8BznOXYQfwR4yAd2Gzay0E3pbK\nXzQnAUD0BiHAm+7PPwIrtdYHtdZFQB6QhlGxLtJa73Af9w9gnrsynw28AaC13g0scR8zDEjC+KaN\n1vo7oBCY3lZGtNargX5a6zKttRNYBgx07z4VeN193D6MCnx/O+lH8mGzz2cAD7s/LwVCMZ5aZgB7\ntdYb3ftuB24CPgYGKaWUO30hRuAUwkOagERv4NBaVzd+Biqa7wMsQCJQ3JiotS5VSpkwnh7igdJm\n5zQeFwuEA1ua6kmiMZpZWqWUCgceV0rNdSfFYzxt4L5XSbM8VBwh/UgONft8EnCXUioRcGI0hZlb\nuXZds7y+i/GE9CJGsFiCEM1IABCBIh+Y1rihlIrDqCgPYlT4Mc2OTQR2YbwnKHM3GXlRSl3axn1u\nxGj6maC1rlBKPQD0de87iFEhN14jHaMSbyu9MXg1imvthkopK8YT0Lla64+VUnagMSAefu1wIN79\npPEaxruTUuAt9xOLEB7SBCQCxRfAbKVUY3PML4HP3W3eyzGaQFBKDQJmuo/ZA+xTSp3j3tdHKfWa\nu129LUnAVnfl3x+jeSfSve8D4BKllEkplQKsxaic20rPA8a47z2wWb4OF+H+r/Hl9A1Anfu+S4EU\npdQk9767gd+7P3+J8TRzPdL8I1ohAUAEBPc33isxXuJuxWj3/4V794NAf6VUNvBX4B33OS7gfODX\n7nO+Ab7SWle2c6vngTlKKY3R8+Zm4Hil1I0Y37YLMALLYuBW9wvcttL/DgxQSm135/GtNspWAvwF\nWKuUWgvsxHh5/SFGU9DZwL+VUtswXoz/1n2eA+PJwQJ8d+Sfogg2JlkPQIjApZS6Heijtb7d33kR\nPY+8AxAiQLlfGF8NnOjvvIieSZqAhAhASqlfYLwzeEhrvcvf+RE9k0+bgNyjD98HHtdaP33YvhOA\nP2H0hPhYa/1Hn2VECCFECz57AnD3pPgr8FUbhzyF8fJqBnCie0i+EEKIbuLLdwC1GF3k7jh8h7vL\n2yGt9V739sfA8RjztLSqsLD8qB9V4uLCKS6uOtrTeyUpc3CQMgeHYylzYmKUqa19PgsA7v7XDc1G\nWDaXgjHkvlEBMKi968XFhRMSYmnvkHYlJkYd9bm9lZQ5OEiZg4MvytxTegG1GaEaHUvET0yMorCw\n/KjP742kzMFByhwcjqXM7QUOf/UC2o/xFNCorztNCCFEN/FLAHDPyBitlBrgnq1xAfC5P/IihBDB\nymdNQO550x8FBgD17vlWPgCytdbvAr/CmKwK4L9a622+yosQQoiWfPkSeDUwt53939Bs9kYhhBDd\nS0YCCyFEkJIAIIQQQUoCgBBC9EDfbtjP2u2FRz7wGPSUcQC91uLFXzF37vFHPO7JJx/lpz89n7S0\nvkc8VgjRs3y1eh8ul4sTJvbr1Hk/bM4nKS6MzNRoAOrqHVhDzKzfUURSXBgRYVYsZhNOp4tFa3PZ\nnVdGWVU9ql8sn67IASAhOpS7rphCjP3oB8K2RQLAMcjL28+XX37WoQBwww23dEOOhBDrth/ky9V7\nue7s0ditR19pVlTXs/9gJUP7xfKfL7Z50k6Y2A+TCSJCrXy0fDd7Cyq44rThbMw+xFer93HpycPY\nX1TJix9tobyqHoBbzhtLQkwo9/xzBYmxYeQebG/NIcjOK/N8Liqr4bFX13DvZZPaOePo9JoFYY5l\nLiBfjRy87bYb2LJlE6WlpZx44ink5e3niSee5cEH76OwsIDq6mouv/xqZsyYxa9/fTU333w7ixZ9\nRWVlBTk5e8jN3cf119/CtGkzujxvMloyOEiZW7r8z18DcM2ZWei9JYTZLZw4KYOq2gZe+mgLU0Yk\nM3tMGgdLq6mqbSAlPpxn393IxuxDZGXGMyQ9hphIOy9/shUAu81CbZ2jxX3GDu7Duh0HfVPIw9x4\n/jhGD2h1yegj8stcQN3tja93sHJrQav7LBYTDkfn48ekYUmce9zgNvdfcMHFvPPOG2RmDiInZzfP\nPvsPiosPMXnyVE45ZQG5ufu4++47mTFjltd5BQX5PPLIU3z//TLef/9tnwQAIXqrQ2U1xEXZMZlM\nuFwuCkuqiQq3YTabCLEYddmeA+X8sDmfQ+U1hNtD+G7jAZLjwvn9pRM913nuvY00/qv/cNkeT7re\nW8Irn+lW770x+xAbsw95pbVW+QPHVPlHhlmpqK73SouJsBFqs5BfXO2Vnhgbyuxx6ZQUt//UcDQC\nJgD42/DhIwGIiopmy5ZNfPDBO5hMZsrKSlscO3r0WACSkpKoqKjo1nwK0R0e++86UhMiuOCEIa3u\nd7lc1NQ5sNssbM4+xGtfbeeq00fw/aZ8Pl+5F4DpWSks23igxblDM2LZllPSIn1fYQVXP7y46R5d\nU5RWzRvXl0VrcwGYPDyJHbmlxETYPU03cVF2HA4nd10ykX9+vIWt7vz+dN4glm/M58afjiYuys7S\nDXmMGdyHyDArZrMR3IrLa/lsRQ4LZw3EbjOasKwhvumvEzAB4NzjBrf5bb07HpOtVisAX3zxKWVl\nZTzzzD8oKyvjyisvbnGsxdLULtlbmuCEaK6sqg4T4HQZL0gXTOuPzWph6YY89hdVer5JZyRHUlPn\nIC7KTm5hBQXF1Zw+M5PdeWU8//4mr2ve9/Iqr+3WKn+g1cq/Pf2SItlb0PKLVpg9hLSEcHbuL2ux\nz2I24XC6OO+4wQzLiMNus7BaFxAbaSevqIqz5wzkxEn9SIwLw2xqamFZuiGP8uo6TpnS35N2+4Xj\nva7dfN+sMWkt7h0XZef841sPnF0tYAKAP5jNZhwO78fDkpISUlPTMJvNLFnyNfX19W2cLUTPVF3b\nQG29g09/yGHz7mJGZsZx7rzBFJRUkxgThtls4tZnvqPB4fI0ZXy4bHer13rxoy0t0r5ro2LvDIvZ\nxGWnDqO61kF5VR0/bCkg/5AxY/DN545h+75SVmzJ56w5g5g0LAmA5ZsO8NLHW/nTVVPoExvmudZH\ny3fz9hJj1cwzZ2XykxmZrd7ztGkDvLaT48NbHDNzdOoxl607SQA4Bv37Z6L1VlJT04iNjQVg7tzj\nuPPOm9m8eSOnnfYTkpKSeOmlv/s5pyLYbdtbQnSEjZT4cOobnJRW1hIbaaeyup5/fLiZPrFhnDq1\nP4mxYdz/yiryipqmX99XWMFnK4xmmVEDE6iorqfB/U7t8HbszlowvT+xkXb+/XnrU4Gdf9xg5k/q\nx6c/5LBKF2A2mfjDVdOoKK8m1NZUff1kZibPvPMjQ9JjyRqYQNbABBbOHuh1rWkjU5g2MuXwW3Da\ntAGcOKkf9Q1OwkOtx1Se3kZ6AQUoKXPgy8kvZ+zwFA4davlyMCe/nJVbC1gwfQCllXXc+fxyACaq\nRLbtLaGsqvWKu39yFHvyj/5nmBgbSmFJTZv7F0wf4HlaOGlyP847zmjqyC+uoqKqnp25pfRNigSg\nb58IYiPtLe8R4L/nBmcDr2z+L1NSJzAyYRhwzOsBBH4vICGCyZY9xTz82lqmZqVw7txBWMwmvt2Q\nx4adRQxMi+bTH4xBRB8t3+N13ird/sjSo6n8545Nw+F0ceEJQ7GGmNmwq4ivVu/jUFkNeUVVPH3j\nLHbklpEcF0ZyfDg/mTGAVVsLmKASPddIjgsnOQ4G9Y3p9P39zeVyYXK/B9hdlkNNQy3D4oe0ur+1\nc9/Z8SH9ovoyOcV4V7Dl0DZWF6xndcF6Th1wAjP7TiUR36yAJk8AAUrK3Ps1jhp1uYyK2eWCBoeT\n/slR/Pk/a7wq6/hoO4fKao/6XoP7xrAjt2WPtUbDMmIZ1j+OEIuZtxbvBODFO+a1WbEB1NQ1UFfv\nJDrCdtT5ak1X/Z7rHPU4XQ5CQ0I9aTnl+9hVsoe5/WZ4Vdzbinfyv12fMjVlIjP6TvEc/+rWt9CH\ndjAiQZEUnshb2z8A4OLh5xIfGseSfd9RUlvGDeN+wSe7v2Rc4igyotM951fWV3H7t/cA8PS8h9h8\nSPPs+n+2yOsb5z3nkycACQABSsrc+xSUVPPD5nxOm9qf0so67nh+OTERVoqOoWJv7pFrpnPrs8s8\n27ecP5Yn3ljPmbMyOW3aAPKKKqmuddAnNpTIUCufrchhvEokOc77ZWdpRS31DU6vF6ndpaq+mtUl\nq5kaPwWrpf32+pLaUqJtUZhNTV0o95TtxYSJjOh07ln+EIXVRUSEhDO//1zm95/LtV/f7jnWbDKz\ncPBpVNdX8/HuLz3pd025BRMmvs9bxRc5izuU76SwPhRUG+MG/jTjbnaU7GR/ZT4JoXH8Z+tbRzz/\n5ulXMSj06HoGSQDo5RXD0ZAy9zwrtuQTbg9hSHosm3YfIjuvjLNmD+TAoSo+XLab5ZvyPccOTY9h\n2762v5EDZCRHUlXrYNTAeBatMfqkzx6ThuoXS0llLW8u2kmfmFBuOX+spxL/+/82sXxTPg9ePZVk\n9wvhEIup3W/y/uZwOnhv58f0i+rLyvy1bC7SmE1mjus3iz5hCYyIV6wt3MDyvFXcPvE67BYbm4o0\nz65/EYDH5tyP3WJjW/FOnlz7AgAz0qbw3f4fvO5z3dir+Ou6ntlh45Kx5zAlfvJRnSsBoIdXDL4g\nZfYfp8vF/oOV/OvTrRw/IZ0fdxZRW+9kzTaj/T3EYvL0ojla07NSuHLBCBITo8jZV8y1j39DqM3C\nX341ncgw45vxobIawuwhhNn996pvfeEm3t/5MdePu5pYewzbinewZN8yEsP6sLM0mwnJYxmXOJpD\nNYd4ZPUzAKRFpHDZyAv5+4+vsGDgSdgsVp7f8HKH7hdisnBq5nw+2PWpJ81usZEZ3Z+txdt9UUSf\ne3reQyQlRUsT0NGe21Mqhu4kZfaPwpJqHv3vOgoOG87fWVkD49m4y5iS4PCngZ+frJielYo1xOzX\nMpfUlhJji6ag+iDf561iQeaJVDVUs+HgJqamTMRitnDT4t9R56xnfNJorsi6yKuJpaMiQsKpbKg6\n8oFdKD40jnum3s71i3/jlR5li6S8rmlQ2a9GX8ZzG15q8zoz0ibz3f4VAETborh61M95ZPXTXsfc\nNeUW4uwxbCrS/HPTfwA4LXM+X+V8w9x+Mzl94EnSC6in6uh00I3WrVtD//4DiIuL92GuRFdwOl18\nsWov07JSiA5vepH5wXfZHCqrISszgYnDkmhwOHnktbVHbLI5EluImeMnpDN7bBrJceF8v+kAn6/c\ny3XnjAbgxQ+3sGD6AAamRXf4mi6XizUFGxgaN4goWyQPrXwKp8vJbybf2OY59c4Gnl3/T8rqyjlQ\nmc+fZtzNCxtexuFycNKA43h/5yecnnkiL21+zeu8z/csYkjsQLaX7GJL0TauyLqIOqfR3XRNwQZK\nVz97FD8Vjqnyj7CGU1nfdH5KeBL1znqKaooBuHPSDYRaQqlx1PB/W96guKaE8cljOC1zPhazhemp\nk1iZv456dzlSI1Ior9sBQKw9hqw+w5mTPp2y2nK2HNpGjaOW89VCXtfvAjAnfQbTUifzyOqnuXDY\n2WTGZPD4nAdYeWANr+q3PXkymUxMSB5DnaOOGHs0IxIUp2bOP+pyd5Q8ARyDvLz9PPPME9x//186\nfM4DD9zDBRdcxMCBbU8y1xV6wrfh7tYVZX7l061EhFk5Y2Ymr3+1na/X5BIbaeOkyRkUl9d65qlp\nblDfaHbmtpxOoNFdl0wkIjSENxbtYO32gyycPZD1Ow5SWlHHkPQYvt+cz/VnjyYzNYqYVvq9t+dI\nZV5fuJG//fgKmdEZ3Drx155v4JeNuIAxiVlYLVaKqosprD6Iw+Xkv/pdTs08gf/b8kan8tEdhsUN\nYVfZHuocdS32jekzknpnA5sPNU3ydsfE6wkNsXPv9w9z9uAFDIkbTEpEEmZM7K88QEltKaP6jOjQ\nvf+95U0anA4WDj6Vz/csIj40jonJ44ixN3XPdDgdbCvZybC4Ifx60R0A/H7qbSSHJ7Z6zf/qd6lx\n1PLzEecf8f6+egKQAHAMGqeDPuusc9m1awfl5eU4HA5uvPE2Bg8ewr///TJLlizCbDYzY8Yshg8f\nwd1330l6egb33/8XUlJajkrsKhIAOqa+wcGHy/YwZ2waUeE2fvHIYgBmj0nlm/V5x5ynZ2+e7Rmx\n6nK5KK+q9+oW6XK5cLnwTATWWc3LvKMkmyfXvsCtE66lf7SxcMkn2V/xYfZnADwx90/cuPi3XudP\nTZ3I93nec/D4wug+I9lwcNORD2xDQmg8902/E4fTwWs73yLZlky0LYoNBzczNXUCWQnDceGiqr6a\nTUVbibXHoOKNL1kOpwOzydytL7qfWfcimw9pHp39R0JDOhfUWyMB4AgB4J0dH7K24MdW9zVO7NRZ\n45JGcdbgBW3uX7NmFe+88waDBw8lIaEPp59+JtnZu3jyyUd44olnWbDgBN5771MsFgvvvfc2Cxee\n41kXQJ4Aul5Hy/zlqr28+uV27vzZeP78nzWe9Ikq8YgDpTqib2IEwzLiOH5COimtzBfTWTUNNdS6\nmwYOl5AQQUFhGRazhT+vfJK95UZvoOTwJG4a/0seWPGYp83aag6h3tlwzPnprDGJWZw9eAGPrH6G\nsrqm30+/yDTSo/oyIXkM8aFx7CzJxuFysuLAanaV7uGOSdfz0sZXKag+yB2Tricjyug/3xv+tp0u\nJ3WOOq8xBsdC3gH0YD/+uIGSkmI+++xjAGprjaHwc+cez403XsP8+Sdz4okn+zOLwq3B4eTVL43e\nIM0rfzjyKNnD/e22uazSBfztg80AnHfcYCaoRCJCrR3ueVPdUE1eZQEDY5pmiCytLae8rpz0KGOm\nyKfW/Z09ZXt5fM792Cw2SmpLeWz1cwxPGMreyn3sKdnX4rr5VQX8a/PrXi8sO1v5m01mzCYzDYed\nNzllPCsOGD+7sJBQqhu8p364dMQFvOx+P3DJ8POYlDIOs8nMgzPvJrcijz+teByAnww6hREJynNe\nY1PJtNSJOFxO7BYb14y5goLqg57Kv7cwm8xdVvn7UsAEgLMGL2jz27qvvzFYrSHcdNNtZGWN9kq/\n9dbfsGfPbr7++guuu+4X/O1v//JZHkRLVTUNLFmXy9xxfXn1i23ERdtJT4zs9HXOnTeY9KQINmcX\nU9vgID7KGHUbYjEzaVgSHy/P4WBpNXPH9vXM396WdQU/MiAmg1i7MeXBCxv+xfaSXdwx6XqKa0oo\nq6vgdf0OAFazlWhbpOeF5f0/PEZRTdNiJUtzv2/3XlsOtT7BGsCEpDHsKd/Hweoir/TZfafzTa4x\nWOzBmXcTaY3A4XR49YYZGa88AeDKrIupaaihf3Q/cspz6RMWT0p4kicATEmd4HX9vpFNs2XaLK2P\nEA4xh3gqpsTwBBLDE9otpzh6ARMA/KFxOugRI7L45pvFZGWNJjt7Fz/8sIwFC87kzTdf47LLruKy\ny65i3bq1VFVVtjqFtDg2NXUNOJ1N23sOlHPvyys922+6py5oTUyEjStOG85jb6wHICU+nGsWZrF9\nbwn/W7abiup65oxNI8weQlZmy4rIYjZz3xXtD9Cpbqjh1m9+33RPWxS/mXwT+VWFbC8xpiHeUZLN\n29v/53Ve894qgFflf6xOHnA8VrOVV7a8zq7SpvmCzlNn0jcyhTpHHZHWCAAsZu+gFm5tataqc9Qx\nNmkUAHGhsZ70+RlzSQhrvafb4NhMdpRkkxjWp8vKI46OBIBj0Hw66Pz8A1xzzZU4nU5uvPFWIiMj\nKSkp5qqrLiEsLJysrNFER8cwdux47rrrDh588FEGDhzk7yL0Wi6Xi+835zNmUAJ/+OdKispqCLVZ\nGJkZz+oONOVcu3AUQ/vFEBVu41BZUxPGn66eCkB6YiTzxnes2WF/xQGe2/AS5w09k6w+wzlQWUBc\naCxr8tcTFxrLh7s+9zq+tK6cO5fe55V2eOXflZ6a+yAf7/6ST3d/5dlurNSvH3s1hdVFPLDiMSYm\nGyvVzew7tc1rxdljGR4/lP5R/dhbkcuQuIGtHnfm4FPbvMa1Y66gsr7KqweN8I+AeQncnt7w0qir\nBVKZ6+odrNleyOThyVTXNlBWWce+wkqee29jp66TEh/OAfeiIX+/fS4WszFHjMvl4uVPtjKsf1yr\n88U3qqirxGwyeX0DPlCZz+Nrnqei3piSOTM6g+yynM4WseNlCE/iouHnthhM1NylIy4guyyHJfu+\nY0SC4toxV+B0OXlp06uM6jPCM+tkc/XOBizuNv/WPL/hZX48uJmbxv+KwbGtL5jSXQLpb7ujpBeQ\nBIBOCaQy/+vTrSxZt/+oz3/ol9NIdE9ctnVPMXUNTkYP6li7stPl5Ik1L5DVZxjv7/wEMPqXl9dX\neuaa6S5D4wZzw7irAaior+SOb+8FYEHmiZyQMYfP9iwiOTyRSSnjcDgdrC5Yz9jEUdiOMGlaR9Q5\n6sirzPd0L/WnQPrb7ijpBSSCxo+7iti+r4RTpvTHbrOws51pig8XERpCZY13r5Wo8KYKcFj/uE7l\npayunJ2l2ewszfakPbTqqU5dozVjE0exvnAjrlaWLk+LSKGktpRxSaM80wg8Pe8hr2Ma2+fjwmI4\nJfMEABYMPNGz32K2tPpN/2jZLLYeUfmLriUBQPhdbb2D7P1lHDhUxdxxfXnc/UL2w2V7mD0mjX2F\nLVe8ajRtZArLNx1g5uhUrj9/PBVl1ewrrMDpdLE1pwQTeC0d2J6S2lK+z1vNpqKtXDXqYnaX5ni9\nIO0qD838A5G2CG795vdUN9QwMXksOeX7uDLrYnLKc8lKGEaULRKXy8UPB9aQGZ3R6iCmh2fdQ3Ji\nDOUlsu60ODoSAITf1Dc4eObdjWzY2dQVcVee95QK36xv2fTTPzmKypp6Fs4ayOQRSUwbmcyIzHjC\n7CFUgKerZ0Zy00vG3Io89pTtY3raJBqcDfxr8+sMis1kbvoMAA5WH+IPy//sOf43S//YoTKc2H8e\nP+StorTZAKfUiGTGJo7ik2ZzyA+LG8LW4u2kR6YRaTO+vf967JV8kv0l56uFhIUYTVTNu0maTCae\nmPNAmyNYw63hhFpDKUcCgDg6EgBEtysoqebO55cTGWZtsaj40g2tT79gCzEzd1xfFs4eiN3q3S0x\na2D77fkOp8Mz+GhAdD9WF6xnTcEG1hRsYHBMJh9mf8aPB7ccVVmmpkzgjEGncN/3j5BfVcDlIy9k\nZMIwQkNCqaiv5Nvc5WRGZ3DduKsoqi4mwtq0iMqA6Ax+Nebydq/fk+fpF72fBADR7T5yLwp+eOXf\n3BWnDWfGqFSKy2sJt4cccYBVa+qdDdy7/C8U15Z40h5Y8ZjXMQ+ufKLT120uPtR4p3DDuF+wryLX\ns4g3YExAFpvJuCRjgGBCWOfePwjhaxIAhM/99e0NOJwurjkzi6/X5PJtG9/yxw7uw7odxrJ507OM\n7pgxkcZiIEPjBnFCxpwW5zhdTswmMweri7jzvfsorzWmPrh32p3sLMn2qvw7akS8orqh2tOdc2zi\nKMJDwjhYc4iCqkLGJY5i0b6lAJ5lCWPsUcTYh3ldx2qxMsHdt16InkgCgPCJ/ENVWCwmlqzbz9rt\nRqX+y0eXtHvONQuzWLmlAJu1aebGxtkdNxVtBcDpdDK//1xMJhObirbyjx//jytHXcKWIu2p/AGv\n9vzOuHPSDfSL6ktNQy2F1UV8lP05J/af69UDZnOR9gQAIXozCQCiS+mcYsqq6js1SGve+L5ceMIQ\nLGYz07K8B2LVOpoWRH93x0dGmrOOtQU/kl9VAMDy/Ss8Uyoci+HxQ+kX1ReA0BA7/aLS+OXoS1sc\nlxnTn7SIFOb1m3XM9xTCnyQAiC7hdLp45PW1bM3peJNLRnIk8yf2Y8aoVK/0PWV7eWrt35meNsmr\nTb1R45QGjdYWtj4NeKOT+h9HXGgsy/evZE+5saDLpSMuYFPRVmakTSYpPImPsj/j9IEdm7E1LCSU\n3025uUPHCtGTSQAQx8zpcnHlXxa1uf+pG2bxwdJsjpuQztPv/MjBkmp+d8lE+iU1zcyZU76PaFsU\nxTWlnmkOvt77LV/v/faY8zeqz3AyY/rjcrnYU76XjKi+TEoZx6SUcZ5jLhx2zjHfR4jeRgKA6LTi\n8lo+W5FDRnIkSzfkERnWNNL2+PHpfLWmaX76M2dlEhlm5cL5QwG47/LJHKop5tkNzxOaYyfOHsNF\nw8/loZXG6NooW+ema/7TjLv57XdGn/2Y0GguHnYuT6/7h9cxme659menTyPaHsUAGdEqBODjAKCU\nehyYCriAG7TWK5vtuxa4CHAAq7TWba9SLfxq/Y6D7Mkv5yczjEnA7nh+OQ0OZ6vHnjVnIIvW5uJ0\nuRg1MIEF0wd47TebTSzOXeoPR8e4AAAeVUlEQVRpv9/DXs/c+IDXAiZHct3Yq4ixR/Ho7D/y6ta3\nOCNrPkXF3gPJ5rgHejUam5jV4esLEeh8FgCUUnOAIVrraUqp4cA/gWnufdHAbcBgrXWDUupzpdRU\nrXX7K1wIv3jyrQ0AHDc+nRVb8tus/AHC7CH84455lFTUEhVuxdxsIFOdo54XNrzM1uLtXucs3vdd\nu/f/6ZAzeHP7+y3Sh8UPAYwXtpdn/YzExCgKXGWcMuAExiSOpKK+kqGxMuW2EG3x5RPA8cB7AFrr\nLUqpOKVUtNa6DKhz/xeplKoAwoGuW+1C+MT1T7Zsjz9pcj9UvzgKiqu8JlozWWv567qXOHPwqRyq\nKWFMn5G8svn1FpX/kZyWOZ+5/WZgNYfwqn77iMebTCavSdGEEG3zZQBIAVY32y50p5VprWuUUvcC\nu4Bq4HWtddvr1wFxceGEhHR+NGijxMTgW3ziaMtc3+AkxGLC4XRRXdv+OrILjxtKX/fcO7llB9he\nopmeMZH/LH+D7SW7eHhV2/PWj0/NYk1eU3fRn41eyMlD5rLz0B7uWWSM2D1lxGwSI6M4M/EEzhhz\nPJsKNPctfrLN8snvOThImbtGd74E9rQFuJuAfgsMBcqAr5VSY7TW69s6ubi46qhvLPOHd9zB0mpu\nf255h4+34aKwsByny8lNi4z56cvLaymuKDvCmRBmCvfa7h86gLLiWhJNTWMBykpqMVU3lcNR1fQl\n4PDyye85OEiZO39uW3wZAPZjfONvlAY0zgEwHNiltT4IoJT6FpgAtBkARNerrm2gqKyGiFArn63I\nYfG6XIZltD1fTXzGQczJu+hXeiLh9lDmjuvr2fdx9heezy9u/Df9ItM6kIOm9wNnDV5AWkTL1bis\nZu8/UUsbK1YJITrPlwHgc+Be4AWl1Hhgv9a6MYTtBoYrpcK01tXAROBjH+ZFtOL3L66gqNl6uIDX\n1MyN/nDpJPolR3LdojsAmDEzhPFJIzz7D1Tm88lhg7P2VnhP45wR1Zec8lyGxA70jNptnEitX1Rf\njs+Y3WoeQ8zSU1kIX/HZvy6t9TKl1Gql1DLACVyrlLoUKNVav6uUehhYpJRqAJZprY99xI/oMIfT\n2aLyb83N542hf4r3I6TL1dQLaHvxTp5Y+8IRr3PHpBsorikhLCSU+75/mNK6ckb1Gc7AmP6kR6a2\ned7hASDSPU4gIVRm1hTiWPn065XW+s7DktY32/cCcOSaQ3S5Nxft4JMf2l64/PHrZtLQ4KS6toHU\nPmEt9uvinYSYrWwq2uJZsrAj4kJjAbh90vXsLc/1WvykLYcvUh5hDefuKbcSY4/u8H2FEK2T5+sg\nUlvv4NMfctqt/P9553Gez9/mLufBxe/yu8k3kxbZ1D7/3f4f+G7/D17nxdpjKKk11u6NskV6Deg6\nfBrnWHuM1+Cv1vx67JUU17Q+r1BKRFK75wohOkYCQBD5aPkePnQvxtJoYFo0Z80eyK6iA2x3LOdg\n9SGqGqrIiErnvR3Ga5nH1zxHVUN1u9fuH5XOTwaeTIw9GrPJxEfZX7CjxFhIfeHg0zqd1+HxQzt9\njhCicyQABAGn08UXq/a2qPwBTpiQTmqKmWd2vQw0zaM/L30myRFJ7Cnb227l3z+qH3vK95IUnsiU\n1Ame9AHRGdy05K4uLYcQomtJAAhgh8pquPXZZS3S779yCovW7aMmbjOxqeXc/8OzLY7p6IInvxpz\nGSvz1zKr7zSv9MaXt42LnQsheh4JAAHqv19q/v3JVu9EcwNjJ1eRHB9Kmirkre3LWbOu44O+AC4a\nfi5hFjt/3/h/gNHef1wrC6OYTWbun/5b7Bb7UZdBCOFbEgACSFllHVHhVkwmk1flv2B6fwqKqynu\nsxRdu4vffreeivrKo7rHtNSJAPx28k04nI52j23s9SOE6JkkAASIPQfKuffllcwcnUpZZZ0n/bRp\n/TlrtjEj5m3fvAnQ6cr/t5Nv4k8rHvdK60gXTiFEzyYBoJfKyS9n8dpczpoziBc/3Mx69wjepRvy\nvI6bPcaYkmFveW67L3MHxgxgV+luz/aFw87m1a3G7Jt9I1O5Mutiz3q5QojAIAGgl/rjv1bhcLpY\nvG5/q/tNYeWEjvqOh3/8hglJY/gmt/22/gvUWTyw4jHP9oy0KZ4AADAuaVTXZFwI0WPIzFq9kNPl\nwuF0tXtMwpBcACrrq45Y+YMxOOuuKbcAMD9jLgAn9p/HvPSZx5ZZIUSPJU8AvdDe/JbLJloS9mPr\nvwVnnR2ryU5CciiVpe1f55whP+Gt7R8AYLNYSY1I5rE593tm4Dxj0CldnnchRM8hTwC9REV1PQAu\nl4v3l2a32J88fC+E1GMOr8ARVkROae4RrzkmcaTns8VkzLNvt9hazL8jhAhM8gTQC3y7fj8vfbKV\n688ezWcr97DTtQJzbBzOkiTMsfmYo4qxW23GIpudEGdv6qZparZ2rxAiOMhXvV7g0xU5YKnnqbfX\no/cXYE3Nxj50DU9cPxP70LVYU3dzoDK/U9e8afyvpNIXIshJAOjhVm4tIK+klLAJX2EbsgZMTS9/\nQ+2dq8CvH3u15/Pg2EzAeNF7fL/WF2MRQgQ2aQLqoRavzeWVzzQApjBj4RZLXCEh1Xs8x9y05Hde\n58TaY7hmzOXklO3j31vf9No3MGaAZxrlrIThnnR50StE8JIA0EM1Vv6Hs6a1fAHcqKS2lL6RqfSN\nTGX20Alc/UHTejwNzgZi7NE8NPMPhFtlgjYhhDQB9ShOl4uPlu9m2cam0bwDhpcxdmJ9p68VGxbD\n7L7TPduNyzhG2iKkl48QApAngB7D5XKxZG0uby/Z1ZjC1OPKWV+xjPzajl1jUvI4r+3z1JmM6jOc\nN7a9x4XDzunaDAshej0JAD3AO9/s8lqsJSRdY03LZn3L8V4emdEZ1DhqyavMx2q2cteUW1pdJ3dE\nguKeaXf4INdCiN5O2gJ6gA9XaMxRhwhJ15ijitpt529kMVsYFDMAAJvZSp+weM8IXiGE6AipMfwg\ntyKPDYWbOC5jNh99vwvb4HVYoouNnR2o/AFSI1JIDEsAYGjcIF9lVQgRwCQA+EHj3PofZn8OgKVl\ny02rpqRM4Hy1kBUH1jAxeSw2i43wkDDGJ4/xVVaFEAFMmoB6gcbFV9Kj0rBZbMzsO5XQkFDMJjPT\n0iZht9j8nEMhRG8kTwDdpN5Rz6J9S1ssnt6exj77LpeLLYe2MSJB+TCHQohgIwGgG1TVV3Hbt/cA\n8H3u2g6fF2mLMD6YIKvP8PYPFkKITpIA4GMHq4v4w/KHPNv5NQfaPNaECRftL/QihBBdRd4B+Niy\n/Ss7dFx4SBjRtigf50YIIZpIAPCxsrryNveNSxpNv0hj0Xany0Wto5MT+gshxDGQANDFnC4n+yuM\nZp6ahlqW57X+BBBji+LnI85niLsPvxMnETJJmxCiG8k7gC722e5FfJj9GadnnM5HW76HiKZ9pqo4\nXOHGgK+xSaOwmkM8E7O5XC6uHXsli/cu7dAi7kIIcawkAHSxNQXrAfhfzv+8Kn9nZTSPzL+VO5bd\nhQsXNrPRd78pADhJDk/kPLWQuf1mEhYS2u15F0IEF2kC6mJOh6XV9IdPuIOIUCsn9Z8HwGj3guwm\njFW9nM16/ySHJ8oLYSGEz8kTQBdyupzkVR7AdFgMSLUMJjLMCsBpA09kdvp0z8ydjU8ATvd8/UII\n0V3kCeAYbSrSfLjrM1wuF+vyN2GyOFoco1JSPJ/NJrPXtM1mWZhdCOEnEgCO0bPrX+ST3V9RXFvC\n85+uaLF/YvJYzhh8cpvnT04ZD8BFw8/1WR6FEKI10gTURf77/SrMsQWe7focxcJZgzl54GxM7XzL\n7xOWwNPzHmr3GCGE8AUJAEfJ5XLxye4vPdsbnV94TevcUNCPUwbN6dC1pPIXQviDNAEdpeV5q/go\n+4s29993acdn/RRCCH/w6ROAUupxYCrgAm7QWq9stq8f8BpgA9ZorX/py7x0tezS3W3u+8use4iw\nhndfZoQQ4ij47AlAKTUHGKK1ngZcATx12CGPAo9qrScDDqVUhq/y0tU2HtzCssOmeKjdPpb6vUM4\nPulUqfyFEL2CL58AjgfeA9Bab1FKxSmlorXWZUopMzALuMC9/1of5qPLPbfhpRZpzuIUfvfziWSm\ndnB9RyGE8DNfBoAUYHWz7UJ3WhmQCJQDjyulxgPfaq1/097F4uLCCQlpfZRtRyQm+mZkbd2eYbgq\no3nk+lmo/vE+ucfR8lWZezIpc3CQMneN7uwFZDrsc1/gSWA38JFS6jSt9UdtnVxcXHXUN05MjKKw\nsO1pmY+kpqGWW765u9V9joIM/nrDHCJCrcd0j652rGXujaTMwUHK3Plz2+LLXkD7Mb7xN0oD8tyf\nDwJ7tNY7tdYO4CtgpA/zckwKq4va3ukyE2aX3rRCiN7HlwHgc+AcAHczz36tdTmA1roB2KWUGuI+\ndgKgfZiXY+J0tZzeoTmZzkEI0Rv57Kur1nqZUmq1UmoZ4ASuVUpdCpRqrd8FbgRedr8Q/hH4n6/y\ncqyqG2paTa/ZKH39hRC9l0/bLrTWdx6WtL7Zvh3ATF/evytsKtI8u/5Fr7Sa9bNwNVi59+czCZfm\nHyFEL9WhJiCl1Ail1IPNtl9SSmX5Lls9x+GVf32OwlUbAQ4b6YkRJMTIwi1CiN6po+8AngE+brb9\nIvB012enZ6vdNp6GA5mebZnDRwjRm3W0/SJEa/1t44bWeqlSKuBrvxZz/TSEEB1uZcSAeCYOS/JP\npoQQoot0NACUKqV+BSzGeGo4GWMgV8ByuVx8fFgAcDmsXHv2KIakx/opV0II0XU62gR0GUZXzTcw\nJnAb7E4LWBsObm6RNmlImlT+QoiA0aEAoLUuBB7SWo/SWo8G/uZOC0j1zga+zFnccofD2u15EUII\nX+lQE5BS6gEgFbjcnXSnUiq7lW6evV5BVSH3fv+wZ9vWEEfZ5pFgcpE8TiZ6E0IEjo42Ac3VWjdW\n/mitz6MX9OHvrOqGGq/KH6B0zRTibX24YMZ4Fkwf4J+MCSGED3Q0ANiUUrbGDaVUJBBw7SFvbfvA\nO8Fp/HiKymo4YWI/rCGygJoQInB0tBfQ88AWpdQqwAJMAp7wWa78ZG9Frte2yykVvhAicHUoAGit\nX1RKbQf6YCzv+AHwG+BxH+at20VZI70TXBIAhBCBq6MvgZ8ATsKY3nkHMAh4xIf56lb1jno+37OI\nGketV3rjE8DVp4/wR7aEEMKnOvoVd4rWejiwTms9CZgPBMzCt5/u/oqPd3/J7rIc7x0uM5edMoyp\nI1NaP1EIIXqxjgaAxq/GdqWUSWu9Gpjhozx1u/L6ytZ3OM2k9Yno3swIIUQ36ehLYK2Uugb4BvhC\nKaWBgBkSazW3/mOwNEQxME36/gshAlNHA8AvgTigBDgfSAYebPeMXsRq9u7RWr36OEJSs7n/J+fL\njJ9CiIDV0V5ALuCQe/NV32XHPyymppaw+v2Z4LDxws9+KZW/ECKgBX0/x3d3fMSne74GYF7kuTTs\nU1x2yjCp/IUQAS/oA8CXOUs8n6uqjP8nxYX5KTdCCNF9gj4ANFde4QIgLsru55wIIYTvBXUAOFCZ\n77W9emMZALGREgCEEIEvqAPAs+tfOizFaPe3WS3dnxkhhOhmQR0AimtLPJ/ja4cBcO68wf7KjhBC\ndKugDgBOl9PzuSZbYTGbmD8p3Y85EkKI7tPRgWABr6jMmO3CYg7qmCiECCJBXduZ3G3+YQ19ALBZ\ng/rHIYQIMkFd48WHGtMZpZTMA+DBq6f5MztCCNGtgjYAuFwuimqKSQlPoqzcSZjdIv3/hRBBJWgD\nwKJ9SwE4UFXAvsJK4qJC/ZwjIYToXkEbAN7e/j+v7WEZATO7tRBCdEhQBoCD1UUt0pLjAmaBMyGE\n6JCgDAB/WtG0lv3s6DMACLXL6F8hRHAJygBQ66jzfA5xRAIQZpMhEUKI4BKUAaC53fsrAAizSwAQ\nQgSXoA8Am3QNADGRNj/nRAghulfQBYDCqsNfAJsYnB5DemKkX/IjhBD+EnQB4MeDm1qkDcuI80NO\nhBDCv4IuAFQ2VDdtGAuAEWaTHkBCiOATdAGgqr4pALhcxmRwTpfLX9kRQgi/8WnXF6XU48BUjO/a\nN2itV7ZyzIPANK31XF/mpVFVQ1WzLSMAVFY3dMethRCiR/HZE4BSag4wRGs9DbgCeKqVY0YAs32V\nh9Y0fwJwlhtt/wPTorszC0II0SP4sgnoeOA9AK31FiBOKXV4Tfso8Dsf5qGFymZPAHU7xhAVbmXi\nsKTuzIIQQvQIvmwCSgFWN9sudKeVASilLgWWALs7crG4uHBCQo7+ZW1iYhQADa76pkSHjXkT+nn2\nBZpALVd7pMzBQcrcNbpz+Kup8YNSKh64DDgB6NuRk4uLq458UBsSE6MoLCwHoMHh9NqXFGP37Ask\nzcscLKTMwUHK3Plz2+LLJqD9GN/4G6UBee7PxwGJwLfAu8B49wtjnzOZTF7b/VOk/V8IEZx8GQA+\nB84BUEqNB/ZrrcsBtNZvaa1HaK2nAguBNVrrm3yYFw9Xsy6fNquZ1HiZBloIEZx8FgC01suA1Uqp\nZRg9gK5VSl2qlFroq3seyb7y/eRXFXi2ByRHYTab2jlDCCECl0/fAWit7zwsaX0rx+wG5voyH40e\nXPmE13bWwITuuK0QQvRIQTcSuDmZAVQIEcyCOgBEhln9nQUhhPCboA0ANRunEREqAUAIEbyCMgCY\nXVZcVTFEyBOAECKIBWUAcLnngY4MlWUghRDBKygDQONCAPIEIIQIZkEZAFy4sNsshFiCsvhCCAEE\naQAAlzT/CCGCXlAGABcuaf4RQgS9oAwAJhPYrLIOsBAiuAVlAADYsa/U31kQQgi/CtoAIIQQwS5o\nA8DFJw71dxaEEMKvgiYA1DnqvLbHDU30U06EEKJnCJoA8PmeRV7bdnkJLIQIckETAA5WF3ttSwAQ\nQgS7oAkAFpN3UWUlMCFEsAueAGAOmqIKIUSHBE2taDZJk48QQjQXRAGgWVFdQVNsIYRoU9DUhM3f\nASTnn+zHnAghRM8QNAGg+RNAhCXWjzkRQoieIWgCgImmXj+h0gVUCCGCJwCEmJsq/TC7rAUghBBB\nEwAanwBqt48lNsru59wIIYT/BU0AcLqcxod6u6wGJoQQBFEAcLgDgMtlYmRmvJ9zI4QQ/hdEAcAB\ngD0khL6JkX7OjRBC+F/QBIDGJiBbiPQAEkIICKIA0NgEZLfKYvBCCAFBFACczZqAhBBCBFEAcDgb\nnwAkAAghBARRAKiprwcgMlTGAAghBARRAKiqqwEgJjzMzzkRQoieIXgCQL2xKHxceLifcyKEED1D\n0ASAOocRAGIj5AlACCEgSAJAQcVBCupzAYiJlHcAQggBQRIAVuSu93yODrf5MSdCCNFzBEUAiLA2\nNftEhctAMCGEAPBpp3il1OPAVMAF3KC1Xtls3zzgQcABaOBKrbXTF/kIs4Z6PssTgBBCGHz2BKCU\nmgMM0VpPA64AnjrskL8B52itZwBRQLcs1Gu3yVxAQggBvm0COh54D0BrvQWIU0pFN9s/QWu9z/25\nEEjwVUYanA0A2AqyfHULIYTodXzZBJQCrG62XehOKwPQWpcBKKVSgROBu9u7WFxcOCFHOZPnxl1G\nAIiwh5OYGHVU1+iNgqmsjaTMwUHK3DW6c2Ic0+EJSqkk4H/ANVrrovZOLi6uOuobV9QYo4Dt5hAK\nC8uP+jq9SWJiVNCUtZGUOThImTt/blt8GQD2Y3zjb5QG5DVuuJuDPgF+p7X+3If54P9+fBOAcLu8\nABZCiEa+fAfwOXAOgFJqPLBfa908hD0KPK61/tSHefASbpdBYEII0chnTwBa62VKqdVKqWWAE7hW\nKXUpUAp8BlwCDFFKXek+5VWt9d98lR+AcLtMBS2EEI18WiNqre88LGl9s8/d8nW8cSlIALutxWsI\nIYQIWgE/EnhD4SbPZ6vN5cecCCFEzxLwAaCwuqlzUZhdBoEJIUSjgA8AKm6w5/OM9PF+zIkQQvQs\nAf9WNCM6nfgD8ykqMBE9T9YCEEKIRgEfAABKCuwkRNkxmeQlsBBCNAr4JqCqmgYqaxpIiA498sFC\nCBFEAj4AHCozpoGQACCEEN4CPgAcbAwAMRIAhBCiuYAPAMXuABAfLdNACCFEcwEfAIakxzJlZAoj\nBsT7OytCCNGjBHwASE+K5K7Lp8hSkEIIcZiADwBCCCFaJwFACCGClAQAIYQIUhIAhBAiSEkAEEKI\nICUBQAghgpQEACGECFISAIQQIkiZXC5ZJlEIIYKRPAEIIUSQkgAghBBBSgKAEEIEKQkAQggRpCQA\nCCFEkJIAIIQQQUoCgBBCBKkQf2fA15RSjwNTARdwg9Z6pZ+z1GWUUn8BZmH8Hh8EVgL/B1iAPOBi\nrXWtUupnwI2AE/ib1vpFP2W5SyilwoCNwB+BrwjwMrvLcjvQAPwe2EAAl1kpFQm8AsQBduBe4ADw\nHMa/4w1a61+5j70N+Kk7/V6t9cd+yfQxUEplAe8Dj2utn1ZK9aODv1+llBV4GegPOIDLtNa7Onrv\ngH4CUErNAYZoracBVwBP+TlLXUYpNQ/IcpftZOAJ4D7gGa31LGAHcLlSKgKj0jgBmAvcpJTq7etj\n3gUccn8O6DIrpRKAPwAzgQXAGQR4mYFLAa21ngecAzyJ8fd9g9Z6BhCjlDpFKZUJnE/Tz+YxpZTF\nT3k+Ku7f218xvsg06szv90KgRGs9E3gA44tghwV0AACOB94D0FpvAeKUUtH+zVKX+Qbjmw9ACRCB\n8YfxgTvtfxh/LFOAlVrrUq11NfAdMKN7s9p1lFLDgBHAR+6kuQR2mU8AvtRal2ut87TWVxP4ZT4I\nJLg/x2EE+8xmT++NZZ4HfKK1rtNaFwJ7MP42epNa4FRgf7O0uXT893s88K772C/p5O880ANAClDY\nbLvQndbraa0dWutK9+YVwMdAhNa61p1WAKTS8mfQmN5bPQrc3Gw70Ms8AAhXSn2glPpWKXU8AV5m\nrfXrQIZSagfGF51bgeJmhwRMmbXWDe4KvbnO/H496VprJ+BSSnV4AfRADwCHM/k7A11NKXUGRgD4\n9WG72iprr/0ZKKUuAZZrrbPbOCTgyoyR9wTgLIymkZfwLk/AlVkpdRGQo7UeDBwH/PuwQwKuzO3o\nbFk79TMI9ACwH+9v/GkYL1UCglLqJOB3wCla61Kgwv2CFKAvRvkP/xk0pvdGpwFnKKW+B64E7ibw\ny5wPLHN/U9wJlAPlAV7mGcBnAFrr9UAY0KfZ/kAsc3Od+Zv2pLtfCJu01nUdvVGgB4DPMV4ioZQa\nD+zXWpf7N0tdQykVAzwMLNBaN74Q/RI42/35bOBT4AdgklIq1t27YgbwbXfntytorc/TWk/SWk8F\n/oHRCyigy4zxN3ycUsrsfiEcSeCXeQdGmzdKqf4YQW+LUmqme/9ZGGX+GjhNKWVTSqVhVIqb/ZDf\nrtaZ3+/nNL0LPB1Y1JkbBfx00EqpPwOzMbpOXev+RtHrKaWuBu4BtjVL/jlGxRiK8ULsMq11vVLq\nHOA2jK5yf9Va/6ebs9vllFL3ALsxvim+QgCXWSn1C4xmPoD7Mbr7BmyZ3RXcP4FkjC7Od2N0A30B\n40vrD1rrm93HXgf8DKPMd2mtv2r1oj2UUmoCxnutAUA9kItRnpfpwO/X3evpH8AQjBfKl2qt93b0\n/gEfAIQQQrQu0JuAhBBCtEECgBBCBCkJAEIIEaQkAAghRJCSACCEEEFKAoAQ3UApdalS6vARrUL4\nlQQAIYQIUjIOQIhm3AOLzsUYgLQV+AvwIfAJMMZ92Pla61yl1GkYU/RWuf+72p0+BWP64jqMmSwv\nwRjReRZQhjFj5R7gLK21/AMUfiNPAEK4KaUmAwuB2e51FkowpuIdCLzknp99MXCLUiocYwTm2e55\n6z/BGKULxuRlV2mt5wBLMOYwAhgJXA1MALKA8d1RLiHaEvArggnRCXOBwcAipRQYayz0BYq01qvd\nx3yHsSrTUCBfa73Pnb4Y+KVSqg8Qq7XeCKC1fgKMdwAY87lXubdzgVjfF0mItkkAEKJJLfCB1toz\ntbZSagCwptkxJoy5WA5vumme3taTdUMr5wjhN9IEJEST74BT3JORoZS6BmPRjTil1Dj3MTMx1uTd\nBiQppTLc6ScA32uti4CDSqlJ7mvc4r6OED2OBAAh3LTWq4BngMVKqaUYTUKlGDM0XqqU+hpjGt7H\n3as4XQH8Vym1GGNpvrvcl7oYeFIptQRjJlrp/il6JOkFJEQ73E1AS7XW6f7OixBdTZ4AhBAiSMkT\ngBBCBCl5AhBCiCAlAUAIIYKUBAAhhAhSEgCEECJISQAQQogg9f8JltmEip/H6wAAAABJRU5ErkJg\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gaZONl1mD8XD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now create a classification report to review the f1-score of the model per class.\n",
        "To do so, we have to:\n",
        "- Create a variable predictions that will contain the model.predict_classes outcome\n",
        "- Convert our y_test (array of strings with our classes) to an array of int called new_Ytest, otherwise it will not be comparable to the predictions by the classification report."
      ]
    },
    {
      "metadata": {
        "id": "EO25uIL-9vqx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict_classes(x_testcnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1i06grlBBSrn",
        "colab_type": "code",
        "outputId": "68316a2b-0b85-4836-a867-2ec793e333d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 5, 4, ..., 2, 5, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "HUHshx93CM_6",
        "colab_type": "code",
        "outputId": "7ddfbb3b-398f-48fb-ccb3-60ffd677e5da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['02', '05', '04', ..., '02', '05', '01'], dtype='<U2')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "tMxojpvWCxOs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_Ytest = y_test.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W07EQaC8DE6i",
        "colab_type": "code",
        "outputId": "b0b027cb-8151-458b-b6fc-c586f37736b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "new_Ytest"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 5, 4, ..., 2, 5, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "FW2XHdTtEedk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Okay, now we can display the classification report:"
      ]
    },
    {
      "metadata": {
        "id": "IfVSRmMu96rC",
        "colab_type": "code",
        "outputId": "6357345c-f287-4032-bc08-5d1eec6f54ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(new_Ytest, predictions)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.88      0.92      0.90       134\n",
            "           2       0.97      0.92      0.94       251\n",
            "           3       0.88      0.89      0.89       242\n",
            "           4       0.88      0.91      0.89       271\n",
            "           5       0.97      0.91      0.94       253\n",
            "           6       0.92      0.93      0.93       239\n",
            "           7       0.89      0.93      0.91       127\n",
            "           8       0.85      0.89      0.87       116\n",
            "\n",
            "   micro avg       0.91      0.91      0.91      1633\n",
            "   macro avg       0.91      0.91      0.91      1633\n",
            "weighted avg       0.91      0.91      0.91      1633\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hu1S5IowfSDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now, the confusion matrix: it will show us the misclassified samples"
      ]
    },
    {
      "metadata": {
        "id": "fdy09SCEd7Cl",
        "colab_type": "code",
        "outputId": "711141ac-e97b-424a-a6dc-ce5f7703f6a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(new_Ytest, predictions)\n",
        "print (matrix)\n",
        "\n",
        "# 01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[123   0   0  11   0   0   0   0]\n",
            " [  2 231   7   5   0   2   0   4]\n",
            " [  8   2 215   2   2   6   3   4]\n",
            " [  4   4   3 246   0   2   4   8]\n",
            " [  0   1   6   3 231   4   6   2]\n",
            " [  0   0   6  10   0 223   0   0]\n",
            " [  0   0   4   0   4   1 118   0]\n",
            " [  2   0   2   2   1   4   2 103]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x_ySPOyHxkZ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ]
    },
    {
      "metadata": {
        "id": "f5kRmoD-sdHj",
        "colab_type": "code",
        "outputId": "d10c6120-ad4a-4134-96d3-1c77bd5ec160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content/drive/My Drive/Ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at /content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MNUiznKNwUtJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Reloading the model to test it"
      ]
    },
    {
      "metadata": {
        "id": "T4oAv6Kx8RBE",
        "colab_type": "code",
        "outputId": "e298ceab-67a1-4c48-aab8-5c8f08e9315e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "loaded_model = keras.models.load_model('/content/drive/My Drive/Ravdess_model/Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 9)                 5769      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 9)                 0         \n",
            "=================================================================\n",
            "Total params: 88,585\n",
            "Trainable params: 88,585\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FHtPzc0Y8hfZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Checking the accuracy of the loaded model"
      ]
    },
    {
      "metadata": {
        "id": "qUi-Zjuf8hDB",
        "colab_type": "code",
        "outputId": "2de8429a-c771-4db2-d28e-330240086d5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1633/1633 [==============================] - 0s 124us/step\n",
            "Restored model, accuracy: 91.24%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8pXH3y7S9A1N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Thank you for your attention! To be continued.."
      ]
    }
  ]
}